#!/usr/bin/env python

###Importing things
import gzip
import pandas as pd
import scipy.sparse as sp
import numpy as np

from Bio import SeqIO
from glob import glob
from itertools import product
from sklearn.externals.joblib import Parallel, delayed
# Tf means term-frequences
# Tfid means term frequency times inverse document-frequency
from sklearn.feature_extraction.text import TfidfVectorizer


###The function
def get_kmers(fn):
    
    # fn = abbrev for file name
    # gzip.open lets me open a file in text mode and returns file object
    ## 'rt' is open for read, text mode (non-binary)
    # SeqIO.parse() takes filename (fn) & format name (fastq) and returns SeqRecord iterator
    ## so we can work through file by file
    
    # Parse will return all the sequences in the fast.
    # .seq returns all the DNA (ignores the fasta header).
    # Will return a vector with the number of rows = number of unique seq. 
    sequences = [str(_.seq) for _ in SeqIO.parse(gzip.open(fn, 'rt'), 'fastq')]

    # Vectorization = turning collection of text documents into numerical feature vectors
    # returns a numpy array (columns = kmers, rows = sequencies)
    # returns a numpy array (with exactly 1 row taking the sum across all rows (above))
    return vectorizer.fit_transform(sequences).sum(axis=0) 


kmer_size = 3
all_kmers = [''.join(_) for _ in product(['a', 'c', 'g', 't'], repeat = kmer_size)]

# create vectorizer class (this is an object):
vectorizer = TfidfVectorizer(
    analyzer = 'char', # count at level of character instead of word. 
    ngram_range = (kmer_size, kmer_size), # can set a range (min, max). The longer kmer is, the less accurate the predicition is. 
    vocabulary = all_kmers,
    use_idf = False, # false makes idf similar to countVectorizer. # TFidf -- frequency of word vs document frequency (i.e. 'the' and 'and'). May not want to do this. 
    #norm = None # scale vector (L1 or L2 norm)
    norm = None # change this to None or l2 later?
)

# -1 means that you use all the CPUs (be very careful, because if one CPU takes a lot of ram, then you can really swamp system.
# instead start with 1 core, guesstimate how much RAM it will take, then scale up. May make sense to use 2-4 cores)
# glob -- grab all *fa.gz files in the current dir (change to fastq), can also specify a dir (glob('home/dir/path/*fastq.gz')).
# Can also do this recursively (glob('home/dir/path/**fastq.gz', recursive=True))
# delayed(get_kmers)(fn). Sets up 'asynchronous' function whereby once a job is done, that core is filled iwth another job.  
# kmers is a list of numpy objects generated by get_kmers

kmers = Parallel(n_jobs = 1)(delayed(get_kmers)(fn) for fn in glob('*fastq.gz'))


# create pandas dframe
sparse_kmers_df = pd.DataFrame(
    # stack the arrays vertically into a sparse (sp) array
    np.vstack(kmers),
    # naming all the columns with the kmers from the vectorizer
    columns = all_kmers,
)

#will tell you how much memory etc. 
#sparse_kmers_df.info()
#print()

print(sparse_kmers_df)
