{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pollard/home/abustion/deep_learning_microbiome/scripts/stats_utils_AEB.py:2: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.\n",
      "  matplotlib.use('Agg') # this suppresses the console for plotting\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from itertools import cycle, product\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, RepeatedStratifiedKFold, train_test_split\n",
    "#from sklearn import cross_validation, metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# import private scripts\n",
    "import load_kmer_cnts_jf\n",
    "import stats_utils_AEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score, precision_score, recall_score\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import operator\n",
    "\n",
    "import pickle\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = os.environ['HOME'] + '/deep_learning_microbiome/tmp_intermediate_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_size = None\n",
    "# number of folds for grid search\n",
    "cv_gridsearch = None\n",
    "# number of folds for actual training of the best model\n",
    "cv_testfolds = None\n",
    "# number of iterations of cross-validation\n",
    "n_iter = None\n",
    "random_state = None\n",
    "\n",
    "plot_fold = False\n",
    "\n",
    "# Per-iteration plotting\n",
    "plot_iter = False\n",
    "# Overall plotting - aggregate results across both folds and iteration\n",
    "plot_overall = True\n",
    "\n",
    "# controls transparency of the CI (Confidence Interval) band around the ROCs as in Pasolli\n",
    "plot_alpha = 0.2\n",
    "\n",
    "# factor for calculating band around the overall ROC with per fold ROCs (see Pasolli).\n",
    "plot_factor = 2.26\n",
    "\n",
    "class_name = \"Disease\"\n",
    "\n",
    "graph_dir = os.environ['HOME'] + '/deep_learning_microbiome/analysis/kmers'\n",
    "\n",
    "\n",
    "plot_title_size = 12\n",
    "plot_text_size = 10\n",
    "\n",
    "dataset_config_iter_fold_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param_grid = {\n",
    "    \"rf\": {'DS': [[\"Zeller_2014\"],[\"Zeller_2014\"]],  'CVT': 10,'N': 20,'M': \"rf\",'CL': [0, 1],\n",
    "             'CR': 'gini', 'MD': None, 'MF': 'log2', 'MS': 5, 'NE': 400, 'NJ': 1, 'KS': 6, 'NR': 1, 'SL':0}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall(precision, recall,  average_precision, f1_score, name ='precision_recall', config = ''):\n",
    "    fig = pylab.figure()\n",
    "\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "         color='b')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    pylab.gca().set_position((.1, .7, 0.8, .8))\n",
    "    add_figtexts_and_save(fig, name, '2-class Precision-Recall curve: AP={0:0.4f}, F1={1:0.4f}'.format(\n",
    "              average_precision, f1_score), config=config)\n",
    "    \n",
    "def plot_confusion_matrix(cm, name = 'confusion_mat', config='', cmap=pylab.cm.Reds):\n",
    "    \"\"\"\n",
    "    This function plots the confusion matrix.\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    \"\"\"\n",
    "    fig, ax = pylab.subplots(1, 2)\n",
    "    for sub_plt, conf_mat, title, fmt in zip(ax, [cm, cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]], ['Unnormalized Confusion Matrix', 'Normalized Confusion Matrix'], ['d', '.2f']):\n",
    "        im = sub_plt.imshow(conf_mat, interpolation='nearest', cmap=cmap)\n",
    "        sub_plt.set_title(title, size=plot_title_size)\n",
    "        divider = make_axes_locatable(sub_plt)\n",
    "        cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "        #fig.colorbar(im, ax=sub_plt)\n",
    "        fig.colorbar(im, cax=cax1)\n",
    "        tick_marks = np.arange(len(cm))\n",
    "        sub_plt.set_xticks(tick_marks)\n",
    "        sub_plt.set_yticks(tick_marks)\n",
    "        sub_plt.set_xticklabels(classes)\n",
    "        sub_plt.set_yticklabels(classes)\n",
    "        sub_plt.tick_params(labelsize=plot_text_size, axis='both')\n",
    "        thresh = 0.8*conf_mat.max()\n",
    "        for i, j in product(range(conf_mat.shape[0]), range(conf_mat.shape[1])):\n",
    "            sub_plt.text(j, i, format(conf_mat[i, j], fmt),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if conf_mat[i, j] > thresh else \"black\", size=plot_title_size)\n",
    "        sub_plt.set_ylabel('True Label', size=plot_text_size)\n",
    "        sub_plt.set_xlabel('Predicted Label', size=plot_text_size)\n",
    "    pylab.tight_layout()\n",
    "    pylab.gca().set_position((.1, 10, 0.8, .8))\n",
    "    add_figtexts_and_save(fig, name , \"Confusion matrix for predicting sample's \" + config + \" status \", y_off=1.3, config=config)\n",
    "    \n",
    "def plot_roc_aucs(fpr, tpr, roc_auc, accs, std_down=None, std_up=None, config='', name= '', title='ROC Curves with AUCs/ACCs', \n",
    "                  desc=\"ROC/AUC plots using 5mers\", xlabel='False Positive Rate', ylabel='True Positive Rate'):\n",
    "    fig = pylab.figure()\n",
    "    lw = 2\n",
    "    if n_classes > 2:\n",
    "        pylab.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                   label='micro-average ROC (AUC = {0:0.4f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "                   color='deeppink', linestyle=':', linewidth=4)\n",
    "        if (std_down is not None) and (std_up is not None):\n",
    "            pylab.fill_between(fpr['micro'], std_down['micro'], std_up['micro'], color='deeppink', lw=0, alpha=plot_alpha)\n",
    "\n",
    "        pylab.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                   label='macro-average ROC (AUC = {0:0.4f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "                   color='navy', linestyle=':', linewidth=4)\n",
    "        if (std_down is not None) and (std_up is not None):\n",
    "            pylab.fill_between(fpr['macro'], std_down['macro'], std_up['macro'], color='navy', lw=0, alpha=plot_alpha)\n",
    "\n",
    "        roc_colors = cycle(['green', 'red', 'purple', 'darkorange'])\n",
    "        for i, color in zip(range(n_classes), roc_colors):\n",
    "            pylab.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                       label='ROC of {0} (AUC = {1:0.4f}, acc={2:0.4f})'\n",
    "                       ''.format(classes[i], roc_auc[i], accs[i]))\n",
    "            if (std_down is not None) and (std_up is not None):\n",
    "                pylab.fill_between(fpr[i], std_down[i], std_up[i], color=color, lw=0, alpha=plot_alpha)\n",
    "    else:\n",
    "        i = 1\n",
    "        pylab.plot(fpr[i], tpr[i], color='darkorange', lw=lw,\n",
    "                   label='ROC of {0} (AUC = {1:0.4f}, acc={2:0.4f})'\n",
    "                   ''.format(classes[i], roc_auc[i], accs[i]))\n",
    "        if (std_down is not None) and (std_up is not None):\n",
    "            pylab.fill_between(fpr[i], std_down[i], std_up[i], color='darkorange', lw=0, alpha=plot_alpha)\n",
    "    \n",
    "    pylab.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    pylab.xlim([0.0, 1.0])\n",
    "    pylab.ylim([0.0, 1.05])\n",
    "    pylab.xlabel(xlabel)\n",
    "    pylab.ylabel(ylabel)\n",
    "    pylab.title(title, size=plot_title_size)\n",
    "    pylab.legend(loc=\"lower right\", prop={'size': plot_text_size})\n",
    "    pylab.gca().set_position((.1, .7, .8, .8))\n",
    "    add_figtexts_and_save(fig, name + \"roc_auc\", desc, config=config)\n",
    "    \n",
    "def add_figtexts_and_save(fig, name, desc, x_off=0.02, y_off=0.56, step=0.04, config=None):\n",
    "    filename = graph_dir + '/' + name + '_' + config + '.png'\n",
    "    pylab.figtext(x_off, y_off, desc)\n",
    "    pylab.savefig(filename , bbox_inches='tight')\n",
    "    pylab.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def class_to_target(cls):\n",
    "    target = np.zeros((n_classes,))\n",
    "    target[class_to_ind[cls]] = 1.0\n",
    "    return target\n",
    "\n",
    "    \n",
    "def config_info(dataset_name, model_name, config,  kmer_size, skip_keys=['DS', 'CL']):\n",
    "    config_info = \"DS:\" + dataset_name\n",
    "    for k in config:              \n",
    "        # skip the specified keys, used for skipping the fold and iteration indices (for aggregating results across them)\n",
    "        if not k in skip_keys:\n",
    "            config_info += '_' + k + ':' +str(get_config_val(k, config))\n",
    "    return config_info\n",
    "\n",
    "def get_config_val(config_key, config):\n",
    "    val = config[config_key]\n",
    "    if type(val) is list:\n",
    "        val = '-'.join([ str(c) for c in val])\n",
    "    return val\n",
    "\n",
    "def get_reverse_complement(kmer):\n",
    "    kmer_rev = ''\n",
    "    for c in kmer:\n",
    "        if c == 'A':\n",
    "            kmer_rev += 'T'\n",
    "        elif c == 'T':\n",
    "            kmer_rev += 'A'\n",
    "        elif c == 'C':\n",
    "            kmer_rev += 'G'\n",
    "        else:\n",
    "            kmer_rev += 'C'\n",
    "\n",
    "    return kmer_rev[::-1]\n",
    "    \n",
    "\n",
    "def get_feature_importances(clf, kmer_imps):\n",
    "    print(\"GETTING FEATURE IMPORTANCES\")\n",
    "    importances = clf.feature_importances_\n",
    "    #std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "    #             axis=0)\n",
    "    for i in range(len(importances)):\n",
    "        kmer_imps[i] += importances[i]\n",
    "    print(\"FINISHED ADDING IMPORTANCES\")\n",
    "    \n",
    "def get_lasso_importances(estimator, kmer_imps):\n",
    "    print(\"GETTING FEATURE IMPORTANCES\")\n",
    "    importances = estimator.coef_\n",
    "    for i in range(len(importances)):\n",
    "        kmer_imps[i] += importances[0][i]\n",
    "    print(\"FINISHED ADDING IMPORTANCES\")\n",
    "    \n",
    "def get_lasso_NMF_importances(estimator, factors):\n",
    "    print(\"GETTING FEATURE IMPORTANCES\")\n",
    "    importances = estimator.coef_\n",
    "    for i in range(len(importances)):\n",
    "        factors[i] += importances[0][i]\n",
    "    print(\"FINISHED ADDING IMPORTANCES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest CRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 'Zeller_2014'\n",
    "kmer_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmers_no_comp = []\n",
    "print(\"GENERATING ALL KMERS CAPS\")\n",
    "all_kmers_caps = [''.join(_) for _ in product(['A', 'C', 'G', 'T'], repeat = kmer_size)]\n",
    "print(\"GENERATED ALL KMERS CAPS\")\n",
    "for kmer in all_kmers_caps:\n",
    "    if get_reverse_complement(kmer) not in kmers_no_comp:\n",
    "        kmers_no_comp.append(kmer)\n",
    "kmer_imps = np.zeros(len(kmers_no_comp))\n",
    "\n",
    "print(\"GENERATED KMERS NO COMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_string = config_info(data_set[0], learn_type, param_grid, kmer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = [data_set]\n",
    "allowed_labels = ['0', '1']\n",
    "kmer_cnts, accessions, labelz, domain_labels = load_kmer_cnts_jf.load_kmers(kmer_size,\n",
    "                                                                            data_set,\n",
    "                                                                            allowed_labels)\n",
    "\n",
    "print(\"LOADED DATASET \" + str(data_set[0]) + \": \" + str(len(kmer_cnts)) + \" SAMPLES\")\n",
    "labelz=np.asarray(labelz)\n",
    "labelz=labelz.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = normalize(kmer_cnts, axis = 1, norm = 'l1')\n",
    "data_normalized, labels = shuffle(data_normalized, labelz, random_state=0) \n",
    "x = data_normalized\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config_iter_fold_results[dataset] = {}\n",
    "config_results = {}\n",
    "model = dataset_model_grid[dataset]\n",
    "param_grid = model_param_grid[model]\n",
    "classes = param_grid[\"CL\"]\n",
    "n_classes = len(classes)\n",
    "global class_to_ind\n",
    "class_to_ind = { classes[i]: i for i in range(n_classes)}\n",
    "print(\"GETTING DATA\")\n",
    "data_set = param_grid[\"DS\"][0]\n",
    "kmer_size = param_grid[\"KS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier(n_estimators=400, max_depth=None, min_samples_split=5, n_jobs=1, max_features='log2')\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=10, n_repeats=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_i, test_i in k_fold.split(x, y):\n",
    "    x_train, y_train = x[train_i], y[train_i]\n",
    "    x_test, y_test = x[test_i], y[test_i]\n",
    "    use_norm = True\n",
    "    print(\"KFOLD CROSS\")\n",
    "    \n",
    "    if use_norm:\n",
    "        sample_mean = x_train.mean(axis=0)\n",
    "        sample_std = x_train.std(axis=0)\n",
    "\n",
    "        # Normalize both training and test samples with the training mean and std\n",
    "        x_train = (x_train - sample_mean) / sample_std\n",
    "        # test samples are normalized using only the mean and std of the training samples\n",
    "        x_test = (x_test - sample_mean) / sample_std\n",
    "                \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "                \n",
    "    estimator.fit(x_train, y_train)\n",
    "    y_test_pred= np.array(estimator.predict_proba(x_test))\n",
    "    print(\"FIT TO ESTIMATOR\")\n",
    "    \n",
    "    get_feature_importances(estimator, kmer_imps)\n",
    "    \n",
    "    conf_mat = confusion_matrix(y_test, np.argmax(y_test_pred, axis=1), labels=range(n_classes))\n",
    "    if plot_fold:\n",
    "        plot_confusion_matrix(conf_mat, config = config_string + \"_IT_\" + str(i) + \"_FO_\" + str(kfold))\n",
    "\n",
    "    # printing the accuracy rates for diagnostics\n",
    "    print(\"Total accuracy for \" + str(len(y_test_pred)) + \" test samples: \" +\n",
    "                      str(np.mean(np.equal(y_test, np.argmax(y_test_pred, axis=1)))))\n",
    "\n",
    "    for cls in classes:\n",
    "        idx = []\n",
    "        for j in range(y_test_pred.shape[0]):\n",
    "            if y_test[j] == cls:\n",
    "                idx.append(j)\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        idx = np.array(idx)\n",
    "        print(\"Accuracy for total of \" + str(len(idx)) + \" \" + str(cls) + \" samples: \" +\n",
    "                          str(np.mean(np.equal(y_test[idx], np.argmax(y_test_pred[idx], axis=1)))))\n",
    "\n",
    "    # Compute ROC curve and AUC for each class - http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    acc = dict()\n",
    "\n",
    "                    \n",
    "    for j in range(n_classes):\n",
    "        fpr[j], tpr[j], _ = roc_curve(y_test, y_test_pred[:, j])\n",
    "        roc_auc[j] = auc(fpr[j], tpr[j])\n",
    "        # Round float 1.0 to integer 1 and 0.0 to 0 in the target vectors, and 1 for max predicted prob\n",
    "        # index being this one (i), 0 otherwise\n",
    "        acc[j] = accuracy_score(np.round(y_test), np.equal(np.argmax(y_test_pred, axis=1), j))\n",
    "                \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), np.argmax(y_test_pred, axis = 1).ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "                \n",
    "    # Then interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for j in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[j], tpr[j])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    if plot_fold:\n",
    "        # plot the ROCs with AUCs\n",
    "        plot_roc_aucs(fpr, tpr, roc_auc, acc, config = config_string + \"_IT:\" + str(i) + \"_FO:\" + str(kfold))\n",
    "\n",
    "    # calculate the accuracy/f1/precision/recall for this test fold - same way as in Pasolli\n",
    "    test_true_label_inds = y_test\n",
    "    test_pred_label_inds = np.argmax(y_test_pred, axis = 1)\n",
    "    accuracy = accuracy_score(test_true_label_inds, test_pred_label_inds)\n",
    "    f1 = f1_score(test_true_label_inds, test_pred_label_inds, pos_label=None, average='weighted')\n",
    "    precision = precision_score(test_true_label_inds, test_pred_label_inds, pos_label=None, average='weighted')\n",
    "    recall = recall_score(test_true_label_inds, test_pred_label_inds, pos_label=None, average='weighted')\n",
    "\n",
    "    print(('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\tfold-perf-metrics for ' + config_string).\n",
    "                format(accuracy, f1, precision, recall, roc_auc[1], roc_auc['macro']))\n",
    "                # the config info for this exp but no fold/iter indices because we need to aggregate stats over them\n",
    "        \n",
    "    config_iter_fold_results = dataset_config_iter_fold_results[dataset]\n",
    "    if config_string not in config_iter_fold_results:\n",
    "        config_iter_fold_results[config_string] = []\n",
    "\n",
    "        # the iteration and fold indices\n",
    "        # extend the list for the iteration if necessary\n",
    "\n",
    "    if len(config_iter_fold_results[config_string]) <= i:\n",
    "        config_iter_fold_results[config_string].append([])\n",
    "\n",
    "    # extend the list for the fold if necessary\n",
    "    if len(config_iter_fold_results[config_string][i]) <= kfold:\n",
    "        config_iter_fold_results[config_string][i].append([])\n",
    "\n",
    "    config_iter_fold_results[config_string][i][kfold] = [conf_mat, [fpr, tpr, roc_auc], classes, [y_test, y_test_pred], [accuracy, f1, precision, recall, roc_auc[1], roc_auc['macro']], shap_values]\n",
    "    fold_results = np.array(config_iter_fold_results[config_string][i])\n",
    "    kfold += 1\n",
    "    logging.info(\"Completed fold \" + str(kfold) + \" of \" + str(cv_testfolds) + \" for iteration \" + str(i) + \" of \" + str(n_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SORTING FEATURE IMPORTANCES\")\n",
    "imps=kmer_imps\n",
    "num_features = -1\n",
    "num_feature_imps = num_features\n",
    "if (num_feature_imps == -1):\n",
    "    num_feature_imps = len(imps)\n",
    "if imps is not None and num_feature_imps > 0:\n",
    "    indices = np.argsort(imps)[::-1][0:num_feature_imps]\n",
    "    imps = imps[indices]\n",
    "    kmers_no_comp = [kmers_no_comp[i] for i in indices]\n",
    "    file = open(graph_dir + \"/feat_imps_rf\" + str(data_set) + str(kmer_size) + \"mers.txt\", \"w\")\n",
    "    for i in range(num_feature_imps):\n",
    "        if imps[i] > 0:\n",
    "            file.write(kmers_no_comp[i] + \"\\t\" + str(imps[i] / (20 * 10)) + \"\\n\")\n",
    "    print(\"END FEATURE IMPORTANCE DUMP\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model on Zeller data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeller_2014\n",
      "LOADED DATASET Zeller_2014: 121 SAMPLES\n"
     ]
    }
   ],
   "source": [
    "data_set = 'Zeller_2014'\n",
    "kmer_size = 5\n",
    "\n",
    "data_set = [data_set]\n",
    "allowed_labels = ['0', '1']\n",
    "kmer_cnts, accessions, labelz, domain_labels = load_kmer_cnts_jf.load_kmers(kmer_size,\n",
    "                                                                            data_set,\n",
    "                                                                            allowed_labels)\n",
    "\n",
    "print(\"LOADED DATASET \" + str(data_set[0]) + \": \" + str(len(kmer_cnts)) + \" SAMPLES\")\n",
    "labelz=np.asarray(labelz)\n",
    "labelz=labelz.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING ALL KMERS CAPS\n",
      "GENERATED ALL KMERS CAPS\n",
      "GENERATED KMERS NO COMP\n"
     ]
    }
   ],
   "source": [
    "kmers_no_comp = []\n",
    "print(\"GENERATING ALL KMERS CAPS\")\n",
    "all_kmers_caps = [''.join(_) for _ in product(['A', 'C', 'G', 'T'], repeat = kmer_size)]\n",
    "print(\"GENERATED ALL KMERS CAPS\")\n",
    "for kmer in all_kmers_caps:\n",
    "    if get_reverse_complement(kmer) not in kmers_no_comp:\n",
    "        kmers_no_comp.append(kmer)\n",
    "kmer_imps = np.zeros(len(kmers_no_comp))\n",
    "\n",
    "print(\"GENERATED KMERS NO COMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = normalize(kmer_cnts, axis = 1, norm = 'l1')\n",
    "data_normalized, labels = shuffle(data_normalized, labelz, random_state=0) \n",
    "x = data_normalized\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=2, n_jobs=1, max_features='sqrt')\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=10, n_repeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 13 test samples: 0.6153846153846154\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 13 test samples: 0.6923076923076923\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 13 test samples: 0.9230769230769231\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 12 test samples: 0.8333333333333334\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 12 test samples: 0.5833333333333334\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 12 test samples: 0.5\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 12 test samples: 0.75\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 12 test samples: 0.5833333333333334\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 11 test samples: 0.7272727272727273\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 11 test samples: 0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "for train_i, test_i in k_fold.split(x, y):\n",
    "    x_train, y_train = x[train_i], y[train_i]\n",
    "    x_test, y_test = x[test_i], y[test_i]\n",
    "    use_norm = True\n",
    "    print(\"KFOLD CROSS\")\n",
    "    \n",
    "    if use_norm:\n",
    "        sample_mean = x_train.mean(axis=0)\n",
    "        sample_std = x_train.std(axis=0)\n",
    "\n",
    "        # Normalize both training and test samples with the training mean and std\n",
    "        x_train = (x_train - sample_mean) / sample_std\n",
    "        # test samples are normalized using only the mean and std of the training samples\n",
    "        x_test = (x_test - sample_mean) / sample_std\n",
    "                \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "                \n",
    "    estimator.fit(x_train, y_train)\n",
    "    y_test_pred= np.array(estimator.predict_proba(x_test))\n",
    "    print(\"FIT TO ESTIMATOR\")\n",
    "    \n",
    "    #print(estimator.feature_importances_)\n",
    "    get_feature_importances(estimator, kmer_imps)\n",
    "    \n",
    "    print(\"Total accuracy for \" + str(len(y_test_pred)) + \" test samples: \" +\n",
    "                      str(np.mean(np.equal(y_test, np.argmax(y_test_pred, axis=1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SORTING FEATURE IMPORTANCES\")\n",
    "imps=kmer_imps\n",
    "num_features = -1\n",
    "num_feature_imps = num_features\n",
    "if (num_feature_imps == -1):\n",
    "    num_feature_imps = len(imps)\n",
    "if imps is not None and num_feature_imps > 0:\n",
    "    indices = np.argsort(imps)[::-1][0:num_feature_imps]\n",
    "    imps = imps[indices]\n",
    "    kmers_no_comp = [kmers_no_comp[i] for i in indices]\n",
    "    file = open(graph_dir + \"/feat_imps_rf\" + str(data_set) + str(kmer_size) + \"mers.txt\", \"w\")\n",
    "    for i in range(num_feature_imps):\n",
    "        if imps[i] > 0:\n",
    "            file.write(kmers_no_comp[i] + \"\\t\" + str(imps[i] / (20 * 10)) + \"\\n\")\n",
    "    print(\"END FEATURE IMPORTANCE DUMP\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External validation on Feng data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 'Feng'\n",
    "kmer_size = 8\n",
    "\n",
    "data_set = [data_set]\n",
    "allowed_labels = ['0', '1']\n",
    "kmer_cnts, accessions, labelz, domain_labels = load_kmer_cnts_jf.load_kmers(kmer_size,\n",
    "                                                                            data_set,\n",
    "                                                                            allowed_labels)\n",
    "\n",
    "print(\"LOADED DATASET \" + str(data_set[0]) + \": \" + str(len(kmer_cnts)) + \" SAMPLES\")\n",
    "labelz=np.asarray(labelz)\n",
    "labelz=labelz.astype(np.int)\n",
    "\n",
    "data_normalized = normalize(kmer_cnts, axis = 1, norm = 'l1')\n",
    "data_normalized, labels = shuffle(data_normalized, labelz, random_state=0) \n",
    "x = data_normalized\n",
    "y = labels\n",
    "\n",
    "sample_mean = x.mean(axis=0)\n",
    "sample_std = x.std(axis=0)\n",
    "\n",
    "# Normalize both training and test samples with the training mean and std\n",
    "x = (x - sample_mean) / sample_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = estimator.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "average_precision = average_precision_score(y, y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y, y_score)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = estimator.predict_proba(x)\n",
    "preds = probs[:,1]\n",
    "\n",
    "from sklearn import metrics \n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso w/o NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 'Zeller_2014'\n",
    "kmer_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING ALL KMERS CAPS\n",
      "GENERATED ALL KMERS CAPS\n",
      "GENERATED KMERS NO COMP\n"
     ]
    }
   ],
   "source": [
    "kmers_no_comp = []\n",
    "print(\"GENERATING ALL KMERS CAPS\")\n",
    "all_kmers_caps = [''.join(_) for _ in product(['A', 'C', 'G', 'T'], repeat = kmer_size)]\n",
    "print(\"GENERATED ALL KMERS CAPS\")\n",
    "for kmer in all_kmers_caps:\n",
    "    if get_reverse_complement(kmer) not in kmers_no_comp:\n",
    "        kmers_no_comp.append(kmer)\n",
    "kmer_imps = np.zeros(len(kmers_no_comp))\n",
    "\n",
    "print(\"GENERATED KMERS NO COMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeller_2014\n",
      "LOADED DATASET Zeller_2014: 121 SAMPLES\n"
     ]
    }
   ],
   "source": [
    "data_set = [data_set]\n",
    "allowed_labels = ['0', '1']\n",
    "kmer_cnts, accessions, labelz, domain_labels = load_kmer_cnts_jf.load_kmers(kmer_size,\n",
    "                                                                            data_set,\n",
    "                                                                            allowed_labels)\n",
    "\n",
    "print(\"LOADED DATASET \" + str(data_set[0]) + \": \" + str(len(kmer_cnts)) + \" SAMPLES\")\n",
    "labelz=np.asarray(labelz)\n",
    "labelz=labelz.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = normalize(kmer_cnts, axis = 1, norm = 'l1')\n",
    "data_normalized, labels = shuffle(data_normalized, labelz, random_state=0) \n",
    "x = data_normalized\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(penalty='l1', solver='saga', max_iter=10, n_jobs=1)\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=10, n_repeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 13 test samples: 0.7692307692307693\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 13 test samples: 0.6923076923076923\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 13 test samples: 0.8461538461538461\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 12 test samples: 0.6666666666666666\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 12 test samples: 0.75\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 12 test samples: 0.75\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 12 test samples: 0.6666666666666666\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 12 test samples: 0.75\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 11 test samples: 0.8181818181818182\n",
      "KFOLD CROSS\n",
      "FIT TO ESTIMATOR\n",
      "GETTING FEATURE IMPORTANCES\n",
      "FINISHED ADDING IMPORTANCES\n",
      "Total accuracy for 11 test samples: 0.7272727272727273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pollard/home/abustion/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/pollard/home/abustion/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/pollard/home/abustion/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/pollard/home/abustion/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/pollard/home/abustion/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/pollard/home/abustion/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/pollard/home/abustion/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/pollard/home/abustion/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/pollard/home/abustion/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/pollard/home/abustion/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "for train_i, test_i in k_fold.split(x, y):\n",
    "    x_train, y_train = x[train_i], y[train_i]\n",
    "    x_test, y_test = x[test_i], y[test_i]\n",
    "    use_norm = True\n",
    "    print(\"KFOLD CROSS\")\n",
    "    \n",
    "    if use_norm:\n",
    "        sample_mean = x_train.mean(axis=0)\n",
    "        sample_std = x_train.std(axis=0)\n",
    "\n",
    "        # Normalize both training and test samples with the training mean and std\n",
    "        x_train = (x_train - sample_mean) / sample_std\n",
    "        # test samples are normalized using only the mean and std of the training samples\n",
    "        x_test = (x_test - sample_mean) / sample_std\n",
    "                \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "                \n",
    "    estimator.fit(x_train, y_train)\n",
    "    y_test_pred= np.array(estimator.predict_proba(x_test))\n",
    "    print(\"FIT TO ESTIMATOR\")\n",
    "    \n",
    "    print(\"GETTING FEATURE IMPORTANCES\")\n",
    "    importances = estimator.coef_\n",
    "    for i in range(len(importances.T)):\n",
    "        kmer_imps[i] += abs(importances[0][i])\n",
    "    print(\"FINISHED ADDING IMPORTANCES\")\n",
    "    \n",
    "    print(\"Total accuracy for \" + str(len(y_test_pred)) + \" test samples: \" +\n",
    "                      str(np.mean(np.equal(y_test, np.argmax(y_test_pred, axis=1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SORTING FEATURE IMPORTANCES\n",
      "END FEATURE IMPORTANCE DUMP\n"
     ]
    }
   ],
   "source": [
    "print(\"SORTING FEATURE IMPORTANCES\")\n",
    "imps=kmer_imps\n",
    "num_features = -1\n",
    "num_feature_imps = num_features\n",
    "if (num_feature_imps == -1):\n",
    "    num_feature_imps = len(imps)\n",
    "if imps is not None and num_feature_imps > 0:\n",
    "    indices = np.argsort(imps)[::-1][0:num_feature_imps]\n",
    "    imps = imps[indices]\n",
    "    kmers_no_comp = [kmers_no_comp[i] for i in indices]\n",
    "    file = open(graph_dir + \"/feat_imps_lasso\" + str(data_set) + str(kmer_size) + \"mers.txt\", \"w\")\n",
    "    for i in range(num_feature_imps):\n",
    "        if imps[i] > 0:\n",
    "            file.write(kmers_no_comp[i] + \"\\t\" + str(imps[i] / (20 * 10)) + \"\\n\")\n",
    "    print(\"END FEATURE IMPORTANCE DUMP\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso with NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = 'Zeller_2014'\n",
    "kmer_size = 8\n",
    "\n",
    "kmers_no_comp=[]\n",
    "for i in range(20):\n",
    "    kmers_no_comp.append(\"Factor\" + str(i) +\": \")\n",
    "factor_imps = np.zeros(len(kmers_no_comp))\n",
    "\n",
    "print(\"GENERATED KMERS NO COMP\")\n",
    "\n",
    "data_set = [data_set]\n",
    "allowed_labels = ['0', '1']\n",
    "kmer_cnts, accessions, labelz, domain_labels = load_kmer_cnts_jf.load_kmers(kmer_size,\n",
    "                                                                            data_set,\n",
    "                                                                            allowed_labels)\n",
    "\n",
    "print(\"LOADED DATASET \" + str(data_set[0]) + \": \" + str(len(kmer_cnts)) + \" SAMPLES\")\n",
    "labelz=np.asarray(labelz)\n",
    "labelz=labelz.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=20\n",
    "data_normalized = normalize(kmer_cnts, axis = 1, norm = 'l1')\n",
    "data_normalized = stats_utils_AEB.NMF_factor(data_normalized, kmer_size, n_components = int(n), \n",
    "                                                     title=(str(data_set) + str(kmer_size) + \"mers\" \n",
    "                                                            + str(n) + \"factors\"))\n",
    "data_normalized, labels = shuffle(data_normalized, labelz, random_state=0)\n",
    "x = data_normalized\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(penalty='l1', solver='saga', max_iter=1000, n_jobs=4)\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=10, n_repeats=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_i, test_i in k_fold.split(x, y):\n",
    "    x_train, y_train = x[train_i], y[train_i]\n",
    "    x_test, y_test = x[test_i], y[test_i]\n",
    "    use_norm = True\n",
    "    print(\"KFOLD CROSS\")\n",
    "    \n",
    "    if use_norm:\n",
    "        sample_mean = x_train.mean(axis=0)\n",
    "        sample_std = x_train.std(axis=0)\n",
    "\n",
    "        # Normalize both training and test samples with the training mean and std\n",
    "        x_train = (x_train - sample_mean) / sample_std\n",
    "        # test samples are normalized using only the mean and std of the training samples\n",
    "        x_test = (x_test - sample_mean) / sample_std\n",
    "                \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "                \n",
    "    estimator.fit(x_train, y_train)\n",
    "    y_test_pred= np.array(estimator.predict_proba(x_test))\n",
    "    print(\"FIT TO ESTIMATOR\")\n",
    "    \n",
    "    print(\"GETTING FEATURE IMPORTANCES\")\n",
    "    importances = estimator.coef_\n",
    "    for i in range(len(importances.T)):\n",
    "        factor_imps[i] += abs(importances[0][i])\n",
    "    print(\"FINISHED ADDING IMPORTANCES\")\n",
    "    \n",
    "    print(\"Total accuracy for \" + str(len(y_test_pred)) + \" test samples: \" +\n",
    "                      str(np.mean(np.equal(y_test, np.argmax(y_test_pred, axis=1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SORTING FEATURE IMPORTANCES\")\n",
    "imps=factor_imps\n",
    "num_features = -1\n",
    "num_feature_imps = num_features\n",
    "if (num_feature_imps == -1):\n",
    "    num_feature_imps = len(imps)\n",
    "if imps is not None and num_feature_imps > 0:\n",
    "    indices = np.argsort(imps)[::-1][0:num_feature_imps]\n",
    "    imps = imps[indices]\n",
    "    kmers_no_comp = [kmers_no_comp[i] for i in indices]\n",
    "    file = open(graph_dir + \"/feat_imps_lasso_NMF\" + str(data_set) + str(kmer_size) + \"mers.txt\", \"w\")\n",
    "    for i in range(num_feature_imps):\n",
    "        if imps[i] > 0:\n",
    "            file.write(kmers_no_comp[i] + \"\\t\" + str(imps[i] / (20 * 10)) + \"\\n\")\n",
    "    print(\"END FEATURE IMPORTANCE DUMP\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nmf = pd.read_csv(\"/pollard/home/abustion/deep_learning_microbiome/tmp_intermediate_files/feat_imps_lasso_NMF['Zeller_2014']6mers.txt\", sep = '\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nmf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y = df_nmf[1], x= df_nmf[0])\n",
    "plt.ylabel(\"abs value beta coef\")\n",
    "plt.xlabel(\"factors\")\n",
    "plt.title(\"Zeller_2014 6mers with NMF 20\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/pollard/home/abustion/deep_learning_microbiome/analysis/NMF/beta_figures/CRC6mers_most_imp_factors.png\", bbox_layout='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
