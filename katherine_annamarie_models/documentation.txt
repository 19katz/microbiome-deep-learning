# set up directories:

~/deep_learning_microbiome/scripts
~/deep_learning_microbiome/analysis
~/deep_learning_microbiome/data
~/deep_learning_microbiome/tmp_intermediate_files


# get metadata:

mysql -h westway.gladstone.internal -p -e"select subject_id, sample_id, run_accession, country, continent,ibd,bmi_type from MetaQuery.run_to_study a join MetaQuery.run_to_sample b using(run_accession) join MetaQuery.sample_to_subject c using(sample_id) join MetaQuery.subject_attributes d using(subject_id) where study_id = 'MetaHIT'" > MetaHIT_ids.txt

#####################
# run kmerizer
#####################

#compile
You can make the following change at the line 124, from
auto fh = fstream(out_path, ios::out | ios::binary)
to
ofstream fh(out_path, ofstream::out | ofstream::binary);

g++ -O3 --std=c++11 -o vfkmrz_fastq vfkmrz_fastq.cpp

gzip -dc /path/exp.fastq.gz | ./vfkmrz_fastq
/pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/joined_fastq_files_hmp_combine_sample_reps/700015245c_1.fastq.gz

gzip -dc /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/joined_fastq_files_hmp_combine_tech_reps/700173119_1.fastq.gz  | ./vfkmrz_fastq > tmp_out/700173119_1_kmer.txt

#include <cstring> -- line 7
g++ -O --std=c++11 -o vfkmrz_bunion vfkmrz_bunion.cpp  

./vfkmrz_bunion -k1 vfkmrz_fastq.out -k2 vfkmrz_fastq.out 

./vfkmrz_bunion -k1 tmp_out/700173119_1_kmer.txt -k2 tmp_out/700173119_1_kmer.txt > tmp_out/700173119_1_kmer_union.txt



g++ -O --std=c++14 -o vfkmrz_match vfkmrz_match.cpp 
g++-7.2 -O3 --std=c++14 -o vfkmrz_match vfkmrz_match.cpp

# 1) vfkmrz_fastq.cpp: pass in the r_len and other parameters as arguments (python). Jason will write. 

# 2) wrapper will autoamtically determine the read length from the fastq file. 

# 3) Compile vfkmrz_match.cpp (either IT will update compiler version or we can install something locally). 




~/miniconda2/bin/python vfkmrz_wrapper.py -1 /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/joined_fastq_files_hmp_combine_sample_reps/700015245c_1.fastq.gz --k 10 --offset 1 > tmpOut.txt


~/miniconda2/bin/python vfkmrz_wrapper.py -1 tmp.fastq.gz --k 10 --offset 1 --compile-overwrite > tmpOut.txt

export VFKMRZPATH=$VFKMRZPATH:/pollard/home/ngarud/deep_learning_microbiome/vfkmrz


~/miniconda2/bin/python vfkmrz_bunion_wrapper.py -k1 tmpOut.txt -k2 tmpOut.txt --k 10 --compile-overwrite > tmp_bunion.txt


~/miniconda2/bin/python vfkmrz_match_wrapper.py --db tmp_bunion.txt --in tmpOut.txt --compile-overwrite --kpr 0

kpr tells us how many kmers for each read. 



# set up kmerizing all the data. Do this in two rounds so that first the dictionary gets made, second the counts are made against the dictionary (match). Must be done linearly. 

#forward
while read file; do
    
    echo $file
    num=`echo $file | wc -c`
    let num-=1
    last=`echo $file | cut -c$num`
    
    if [ $last == 'c' ]
    then
	dir='joined_fastq_files_hmp_combine_sample_reps'
    else 
	dir='joined_fastq_files_hmp_combine_tech_reps'
    fi

~/miniconda2/bin/python vfkmrz_wrapper.py -1 /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/${dir}/${file}_1.fastq.gz --k 10 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1.txt

~/miniconda2/bin/python vfkmrz_bunion_wrapper.py -k1 ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1.txt -k2 ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1_bunion.txt --k 10 --compile-overwrite >   ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1_bunion_intermediate.txt

mv ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1_bunion_intermediate.txt ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1_bunion.txt

done < /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/HMP_samples_314.txt

# reverse
while read file; do
echo $file

    echo $file
    num=`echo $file | wc -c`
    let num-=1
    last=`echo $file | cut -c$num`
    
    if [ $last == 'c' ]
    then
	dir='joined_fastq_files_hmp_combine_sample_reps'
    else 
	dir='joined_fastq_files_hmp_combine_tech_reps'
    fi

~/miniconda2/bin/python vfkmrz_wrapper.py -1 /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/${dir}/${file}_2.fastq.gz --k 10 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_2.txt

echo 'bunion\n'

~/miniconda2/bin/python vfkmrz_bunion_wrapper.py -k1 ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_2.txt -k2 ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_2_bunion.txt --k 10 --compile-overwrite >   ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_2_bunion_intermediate.txt

echo 'moving the file\n'

mv ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_2_bunion_intermediate.txt ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_2_bunion.txt


done < /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/HMP_samples_314.txt


# It turns out that ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1_bunion.txt contains all the 10mers possible after kmerizing the first file.

######################
# repeat for 20mers  #
######################

# set up kmerizing all the data. Do this in two rounds so that first the dictionary gets made, second the counts are made against the dictionary (match). Must be done linearly. 

#forward
while read file; do
    
    echo $file
    num=`echo $file | wc -c`
    let num-=1
    last=`echo $file | cut -c$num`
    
    if [ $last == 'c' ]
    then
	dir='joined_fastq_files_hmp_combine_sample_reps'
    else 
	dir='joined_fastq_files_hmp_combine_tech_reps'
    fi

~/miniconda2/bin/python vfkmrz_wrapper.py -1 /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/${dir}/${file}_1.fastq.gz --k 20 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_1.txt

~/miniconda2/bin/python vfkmrz_bunion_wrapper.py -k1 ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_1.txt -k2 ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_1_bunion.txt --k 20 --compile-overwrite >   ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_1_bunion_intermediate.txt

mv ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_1_bunion_intermediate.txt ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_1_bunion.txt

du -h ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_1_bunion.txt 

done < /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/HMP_samples_314.txt


#reverse
while read file; do
    
    echo $file
    num=`echo $file | wc -c`
    let num-=1
    last=`echo $file | cut -c$num`
    
    if [ $last == 'c' ]
    then
	dir='joined_fastq_files_hmp_combine_sample_reps'
    else 
	dir='joined_fastq_files_hmp_combine_tech_reps'
    fi

~/miniconda2/bin/python vfkmrz_wrapper.py -1 /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/${dir}/${file}_2.fastq.gz --k 20 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_2.txt

~/miniconda2/bin/python vfkmrz_bunion_wrapper.py -k1 ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_2.txt -k2 ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_2_bunion.txt --k 20 --compile-overwrite >   ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_2_bunion_intermediate.txt

mv ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_2_bunion_intermediate.txt ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_2_bunion.txt

done < /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/HMP_samples_314.txt


######################
# repeat for 5mers  #
######################
file=700013715
dir='joined_fastq_files_hmp_combine_tech_reps'

~/miniconda2/bin/python vfkmrz_wrapper.py -1 /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/${dir}/${file}_1.fastq.gz --k 5 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_1.txt

~/miniconda2/bin/python vfkmrz_bunion_wrapper.py -k1 ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_1.txt -k2 ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1_bunion.txt --k 5 --compile-overwrite >   ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_1_bunion_intermediate.txt

mv ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_1_bunion_intermediate.txt ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_1_bunion.txt

### all the 5mers needed are in tmp_5mers_1_bunion.txt ###






##############################
# Set up the match script:   #
##############################
##########
# HMP    #
##########

##########
# 10mers #
##########
#forward
while read file; do
echo $file

    echo $file
    num=`echo $file | wc -c`
    let num-=1
    last=`echo $file | cut -c$num`
    
    if [ $last == 'c' ]
    then
	dir='joined_fastq_files_hmp_combine_sample_reps'
    else 
	dir='joined_fastq_files_hmp_combine_tech_reps'
    fi

~/miniconda2/bin/python vfkmrz_wrapper.py -1 /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/${dir}/${file}_1.fastq.gz --k 10 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1.txt

~/miniconda2/bin/python vfkmrz_match_wrapper.py --db ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1_bunion.txt --in ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1.txt --compile-overwrite --kpr 0 > /pollard/home/ngarud/deep_learning_microbiome/data/10mers/${file}_1_10mers.txt

done < /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/HMP_samples_314.txt



#reverse
while read file; do
echo $file

    echo $file
    num=`echo $file | wc -c`
    let num-=1
    last=`echo $file | cut -c$num`
    
    if [ $last == 'c' ]
    then
	dir='joined_fastq_files_hmp_combine_sample_reps'
    else 
	dir='joined_fastq_files_hmp_combine_tech_reps'
    fi

~/miniconda2/bin/python vfkmrz_wrapper.py -1 /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/${dir}/${file}_2.fastq.gz --k 10 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_2.txt

~/miniconda2/bin/python vfkmrz_match_wrapper.py --db ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1_bunion.txt --in ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_2.txt --compile-overwrite --kpr 0 > /pollard/home/ngarud/deep_learning_microbiome/data/10mers/${file}_2_10mers.txt

done < /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/HMP_samples_314.txt

# cut the files and gzip

while read file; do
    echo $file

    cat /pollard/home/ngarud/deep_learning_microbiome/data/10mers/HMP/${file}_1_10mers.txt | cut -f2 | gzip > /pollard/home/ngarud/deep_learning_microbiome/data/10mers/HMP/${file}_1_10mers.gz

    cat /pollard/home/ngarud/deep_learning_microbiome/data/10mers/HMP/${file}_2_10mers.txt | cut -f2 | gzip > /pollard/home/ngarud/deep_learning_microbiome/data/10mers/HMP/${file}_2_10mers.gz

done < /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/HMP_samples_314.txt



##########
# 5mers #
##########
#forward
while read file; do
echo $file

    echo $file
    num=`echo $file | wc -c`
    let num-=1
    last=`echo $file | cut -c$num`
    
    if [ $last == 'c' ]
    then
	dir='joined_fastq_files_hmp_combine_sample_reps'
    else 
	dir='joined_fastq_files_hmp_combine_tech_reps'
    fi

~/miniconda2/bin/python vfkmrz_wrapper.py -1 /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/${dir}/${file}_1.fastq.gz --k 5 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_1.txt

~/miniconda2/bin/python vfkmrz_match_wrapper.py --db ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_1_bunion.txt --in ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_1.txt --compile-overwrite --kpr 0 > /pollard/home/ngarud/deep_learning_microbiome/data/5mers/HMP/${file}_1_5mers.txt

done < /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/HMP_samples_314.txt



#reverse
while read file; do
echo $file

    echo $file
    num=`echo $file | wc -c`
    let num-=1
    last=`echo $file | cut -c$num`
    
    if [ $last == 'c' ]
    then
	dir='joined_fastq_files_hmp_combine_sample_reps'
    else 
	dir='joined_fastq_files_hmp_combine_tech_reps'
    fi

~/miniconda2/bin/python vfkmrz_wrapper.py -1 /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/${dir}/${file}_2.fastq.gz --k 5 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_2.txt

~/miniconda2/bin/python vfkmrz_match_wrapper.py --db ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_1_bunion.txt --in ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_2.txt --compile-overwrite --kpr 0 > /pollard/home/ngarud/deep_learning_microbiome/data/5mers/HMP/${file}_2_5mers.txt

done < /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/HMP_samples_314.txt



########################
# Qin et al. data set: #
########################
ls /pollard/shattuck0/snayfach/metagenomes/T2D/fastq/*gz | cut -f8 -d'/' | cut -f1 -d'_' | uniq > ~/deep_learning_microbiome/Qin_et_al_samples.txt

###########
# 10mers: #
###########

#forward
while read file; do
    echo $file
    dir=/pollard/shattuck0/snayfach/metagenomes/T2D/fastq/

~/miniconda2/bin/python vfkmrz_wrapper.py -1 ${dir}/${file}_1.fastq.gz --k 10 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_Qin_1.txt

~/miniconda2/bin/python vfkmrz_match_wrapper.py --db ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1_bunion.txt --in ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_Qin_1.txt --compile-overwrite --kpr 0 > /pollard/home/ngarud/deep_learning_microbiome/data/10mers/Qin_et_al/${file}_1_10mers.txt

done <  ~/deep_learning_microbiome/Qin_et_al_samples.txt 


#reverse
while read file; do
    echo $file
    dir=/pollard/shattuck0/snayfach/metagenomes/T2D/fastq/

~/miniconda2/bin/python vfkmrz_wrapper.py -1 ${dir}/${file}_2.fastq.gz --k 10 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_Qin_2.txt

~/miniconda2/bin/python vfkmrz_match_wrapper.py --db ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1_bunion.txt --in ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_Qin_2.txt --compile-overwrite --kpr 0 > /pollard/home/ngarud/deep_learning_microbiome/data/10mers/Qin_et_al/${file}_2_10mers.txt

done <  ~/deep_learning_microbiome/Qin_et_al_samples.txt 



while read file; do
    echo $file

    cat /pollard/home/ngarud/deep_learning_microbiome/data/10mers/Qin_et_al/${file}_1_10mers.txt | cut -f2 | gzip > /pollard/home/ngarud/deep_learning_microbiome/data/10mers/Qin_et_al/${file}_1_10mers.gz

    cat /pollard/home/ngarud/deep_learning_microbiome/data/10mers/Qin_et_al/${file}_2_10mers.txt | cut -f2 | gzip > /pollard/home/ngarud/deep_learning_microbiome/data/10mers/Qin_et_al/${file}_2_10mers.gz

done <  ~/deep_learning_microbiome/Qin_et_al_samples.txt


#############
# 20mers    #
#############
#forward
while read file; do
    echo $file
    dir=/pollard/shattuck0/snayfach/metagenomes/T2D/fastq/

~/miniconda2/bin/python vfkmrz_wrapper.py -1 ${dir}/${file}_1.fastq.gz --k 20 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_Qin_1.txt

~/miniconda2/bin/python vfkmrz_match_wrapper.py --db ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_1_bunion.txt --in ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_Qin_1.txt --compile-overwrite --kpr 0 > /pollard/home/ngarud/deep_learning_microbiome/data/20mers/Qin_et_al/${file}_1_20mers.txt

done <  ~/deep_learning_microbiome/Qin_et_al_samples.txt 


#reverse
while read file; do
    echo $file
    dir=/pollard/shattuck0/snayfach/metagenomes/T2D/fastq/

~/miniconda2/bin/python vfkmrz_wrapper.py -1 ${dir}/${file}_2.fastq.gz --k 20 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_Qin_2.txt

~/miniconda2/bin/python vfkmrz_match_wrapper.py --db ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_1_bunion.txt --in ~/deep_learning_microbiome/tmp_intermediate_files/tmp_20mers_Qin_2.txt --compile-overwrite --kpr 0 > /pollard/home/ngarud/deep_learning_microbiome/data/20mers/Qin_et_al/${file}_2_20mers.txt

done <  ~/deep_learning_microbiome/Qin_et_al_samples.txt 




#############
# 5mers    #
#############
#forward
while read file; do
    echo $file
    dir=/pollard/shattuck0/snayfach/metagenomes/T2D/fastq/

~/miniconda2/bin/python vfkmrz_wrapper.py -1 ${dir}/${file}_1.fastq.gz --k 5 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_Qin_1.txt

~/miniconda2/bin/python vfkmrz_match_wrapper.py --db ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_1_bunion.txt --in ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_Qin_1.txt --compile-overwrite --kpr 0 > /pollard/home/ngarud/deep_learning_microbiome/data/5mers/Qin_et_al/${file}_1_5mers.txt

done <  ~/deep_learning_microbiome/Qin_et_al_samples.txt 


#reverse
while read file; do
    echo $file
    dir=/pollard/shattuck0/snayfach/metagenomes/T2D/fastq/

~/miniconda2/bin/python vfkmrz_wrapper.py -1 ${dir}/${file}_2.fastq.gz --k 5 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_Qin_2.txt

~/miniconda2/bin/python vfkmrz_match_wrapper.py --db ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_1_bunion.txt --in ~/deep_learning_microbiome/tmp_intermediate_files/tmp_5mers_Qin_2.txt --compile-overwrite --kpr 0 > /pollard/home/ngarud/deep_learning_microbiome/data/5mers/Qin_et_al/${file}_2_5mers.txt

done <  ~/deep_learning_microbiome/Qin_et_al_samples.txt 




#################################
# Rheumatoid Arthirtis data set:#
#################################

ls /pollard/shattuck0/snayfach/metagenomes/RheumatoidArthritis/fastq/*_1.fastq.gz | cut -f8 -d'/' | cut -f1 -d'_' | uniq > ~/deep_learning_microbiome/RheumatoidArthritis_samples.txt



#forward
while read file; do
    echo $file
    dir=/pollard/shattuck0/snayfach/metagenomes/RheumatoidArthritis/fastq/

~/miniconda2/bin/python vfkmrz_wrapper.py -1 ${dir}/${file}_1.fastq.gz --k 10 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_RA_1.txt

~/miniconda2/bin/python vfkmrz_match_wrapper.py --db ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1_bunion.txt --in ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_RA_1.txt --compile-overwrite --kpr 0 > /pollard/home/ngarud/deep_learning_microbiome/data/10mers/RA/${file}_1_10mers.txt

done <  ~/deep_learning_microbiome/RheumatoidArthritis_samples.txt 



#reverse
while read file; do
    echo $file
    dir=/pollard/shattuck0/snayfach/metagenomes/RheumatoidArthritis/fastq/

~/miniconda2/bin/python vfkmrz_wrapper.py -1 ${dir}/${file}_2.fastq.gz --k 10 --offset 1 --compile-overwrite > ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_RA_2.txt

~/miniconda2/bin/python vfkmrz_match_wrapper.py --db ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_1_bunion.txt --in ~/deep_learning_microbiome/tmp_intermediate_files/tmp_10mers_RA_2.txt --compile-overwrite --kpr 0 > /pollard/home/ngarud/deep_learning_microbiome/data/10mers/RA/${file}_2_10mers.txt

done <  ~/deep_learning_microbiome/RheumatoidArthritis_samples.txt 



##########################################
# try out code from LSA (cleary et al.)  #
##########################################

# try this out on Qin et al. because the files are smaller
dir=/pollard/shattuck0/snayfach/metagenomes/T2D/fastq/
python LSFScripts/setupDirs.py -i $dir -n 726

./miniconda2/bin/conda install gensim



####################
# Jellyfish
####################
file=700013715
dir='joined_fastq_files_hmp_combine_tech_reps'
in_file_1=/pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/${dir}/${file}_1.fastq.gz

in_file_2=/pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/${dir}/${file}_2.fastq.gz

jellyfish count /dev/fd/0 -m 5 -s 100M -t 2 -C -F 2 -o test_5mer.jf < (zcat $in_file_1) < (zcat $in_file_2)

nohup nice zcat $in_file | jellyfish count /dev/fd/0 -m 5 -s 100M -t 2 -C -o test_5mer.jf &
nohup nice zcat $in_file | jellyfish count /dev/fd/0 -m 20 -s 10000000000 -t 2 -C -o test_20mer.jf &


#5mers on Qin et al:

while read file; do
    echo $file
    dir=/pollard/shattuck0/snayfach/metagenomes/T2D/fastq/
    jellyfish count < (zcat ${dir}/${file}_1.fastq.gz) < (zcat ${dir}/${file}_2.fastq.gz) /dev/fd/0 -m 5 -s 100M -t 2 -C -F 2 -o ${file}_5mer.jf
done <  ~/deep_learning_microbiome/Qin_et_al_samples.txt 



# Run Jellyfish on Qin et al.
python ~/deep_learning_microbiome/scripts/jellyfish_wrapper.py

nohup nice bash ~/deep_learning_microbiome/tmp_intermediate_files/all_commands_qin_10mers.sh &


# manually do this, but included in next round of jellyfish -- dump the output. 

while read file; do
    jellyfish dump ~/deep_learning_microbiome/data/5mers_jf/Qin_et_al/${file}_5mer.jf | grep '>' | gzip > ~/deep_learning_microbiome/data/5mers_jf/Qin_et_al/${file}_5mer.gz
done <  ~/deep_learning_microbiome/Qin_et_al_samples.txt 

while read file; do
    jellyfish dump ~/deep_learning_microbiome/data/10mers_jf/Qin_et_al/${file}_10mer.jf | grep '>' | gzip > ~/deep_learning_microbiome/data/10mers_jf/Qin_et_al/${file}_10mer.gz
done <  ~/deep_learning_microbiome/Qin_et_al_samples.txt 


# check if all the kmers are outputted. If not, I need to then add in a zero
data_set='Qin_et_al'
kmer_size=10
while read file; do
    echo $file
    jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
    num_lines=`wc -l tmpOut.txt | cut -f1 -d' '`
    if [ $num_lines = 1049600 ]
	then
	echo 'file is ok...'
	    #cat tmpOut.txt | grep '>' | gzip > ~/deep_learning_microbiome/data/10mers_jf/${data_set}/${file}_${kmer_size}mer.gz
	else
	echo 'filling in zeros...'
	# rerun jellyfish
	dir='/pollard/shattuck0/snayfach/metagenomes/T2D/fastq'
	jellyfish count <(zcat ${dir}/${file}_1.fastq.gz) <(zcat ${dir}/${file}_2.fastq.gz) /dev/fd/0 -m $kmer_size -s 100M -t 2 -C -F 2 -o ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf
	jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
	# run a script to find the missing kmers and fill in a zero
	~/miniconda3/bin/python3 ~/deep_learning_microbiome/scripts/fill_in_zeros.py $file $kmer_size $data_set
    fi
    
done <  ~/deep_learning_microbiome/Qin_et_al_samples.txt 

# One Qin sample was not inlcuded. Run Jellyfish on this last sample: 
dir='/pollard/home/ngarud/shattuck/metagenomic_fastq_files/T2D/fastq_files'
file=SRR1778450
data_set=Qin_et_al
for kmer_size in 5 6 7 8 10; do
    #jellyfish count <(zcat ${dir}/${file}_1.fastq.gz) <(zcat ${dir}/${file}_2.fastq.gz) /dev/fd/0 -m $kmer_size -s 100M -t 2 -C -F 2 -o ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf

    jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/Qin_et_al/${file}_${kmer_size}mer.jf | grep '>' | gzip > ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.gz
done


# Run Jellyfish on HMP
cat /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/HMP_samples_314.txt | grep 'c' >  /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/HMP_samples_314_c.txt

cat /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/HMP_samples_314.txt | grep -v 'c' >  /pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/HMP_samples_314_no_c.txt

python ~/deep_learning_microbiome/scripts/jellyfish_wrapper_HMP_no_c.py
python ~/deep_learning_microbiome/scripts/jellyfish_wrapper_HMP_c.py


data_set='HMP'
kmer_size=5
while read file; do
    echo $file
    jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
    num_lines=`wc -l tmpOut.txt | cut -f1 -d' '`
    if [ $num_lines = 1049600 ]
	then
	echo 'file is ok...'
	    #cat tmpOut.txt | grep '>' | gzip > ~/deep_learning_microbiome/data/10mers_jf/${data_set}/${file}_${kmer_size}mer.gz
	else
	echo 'filling in zeros...'
	# rerun jellyfish
	dir='/pollard/home/ngarud/BenNanditaProject/MIDAS_intermediate_files_hmp/joined_fastq_files_hmp_combine_tech_reps/'
	jellyfish count <(zcat ${dir}/${file}_1.fastq.gz) <(zcat ${dir}/${file}_2.fastq.gz) /dev/fd/0 -m $kmer_size -s 100M -t 2 -C -F 2 -o ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf
	jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
	# run a script to find the missing kmers and fill in a zero
	~/miniconda3/bin/python3 ~/deep_learning_microbiome/scripts/fill_in_zeros.py $file $kmer_size $data_set
    fi
    
done <  ~/deep_learning_microbiome/RheumatoidArthritis_samples.txt 




# Jellyfish on RA
python ~/deep_learning_microbiome/scripts/jellyfish_wrapper_RA.py


# check if all the kmers are outputted. If not, I need to then add in a zero
data_set='RA'
kmer_size=7
while read file; do
    echo $file
    jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
    num_lines=`wc -l tmpOut.txt | cut -f1 -d' '`
    if [ $num_lines = 16384 ]
	then
	echo 'file is ok...'
	    #cat tmpOut.txt | grep '>' | gzip > ~/deep_learning_microbiome/data/10mers_jf/${data_set}/${file}_${kmer_size}mer.gz
	else
	echo 'filling in zeros...'
	# rerun jellyfish
	dir='/pollard/shattuck0/snayfach/metagenomes/RheumatoidArthritis/fastq'
	jellyfish count <(zcat ${dir}/${file}_1.fastq.gz) <(zcat ${dir}/${file}_2.fastq.gz) /dev/fd/0 -m $kmer_size -s 100M -t 2 -C -F 2 -o ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf
	jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
	# run a script to find the missing kmers and fill in a zero
	~/miniconda3/bin/python3 ~/deep_learning_microbiome/scripts/fill_in_zeros.py $file $kmer_size $data_set
    fi
    
done <  ~/deep_learning_microbiome/RheumatoidArthritis_samples.txt 


# Jellyfish on metaHIT
MetaHIT_accessions_only.txt
python ~/deep_learning_microbiome/scripts/jellyfish_wrapper_MetaHIT.py

# check if all the kmers are outputted. If not, I need to then add in a zero
data_set='MetaHIT'
kmer_size=7
while read file; do
    echo $file
    jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
    num_lines=`wc -l tmpOut.txt | cut -f1 -d' '`
    if [ $num_lines = 16384 ]
	then
	echo 'file is ok...'
	    #cat tmpOut.txt | grep '>' | gzip > ~/deep_learning_microbiome/data/10mers_jf/${data_set}/${file}_${kmer_size}mer.gz
	else
	echo 'filling in zeros...'
	# rerun jellyfish
	dir='/pollard/shattuck0/snayfach/metagenomes/MetaHIT/fastq'
	jellyfish count <(zcat ${dir}/${file}_1.fastq.gz) <(zcat ${dir}/${file}_2.fastq.gz) /dev/fd/0 -m $kmer_size -s 100M -t 2 -C -F 2 -o ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf
	jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
	# run a script to find the missing kmers and fill in a zero
	~/miniconda3/bin/python3 ~/deep_learning_microbiome/scripts/fill_in_zeros.py $file $kmer_size $data_set
    fi
    
done <  ~/deep_learning_microbiome/MetaHIT_accessions_only.txt 


# combine technical replicates for MetaHIT
python combine_kmerized_files_MetaHIT.py




# Jellyfish on Feng et al. 
python ~/deep_learning_microbiome/scripts/jellyfish_wrapper_Feng.py

# check if all the kmers are outputted. If not, I need to then add in a zero
data_set='Feng'
kmer_size=10
while read file; do
    echo $file
    jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
    num_lines=`wc -l tmpOut.txt | cut -f1 -d' '`
    if [ $num_lines = 1049600 ]
	then
	echo 'file is ok...'
	    #cat tmpOut.txt | grep '>' | gzip > ~/deep_learning_microbiome/data/10mers_jf/${data_set}/${file}_${kmer_size}mer.gz
	else
	echo 'filling in zeros...'
	# rerun jellyfish
	dir='/pollard/home/ngarud/shattuck/metagenomic_fastq_files/Feng_2015/combined_fastq_files'
	jellyfish count <(zcat ${dir}/${file}_1.fastq.gz) <(zcat ${dir}/${file}_2.fastq.gz) /dev/fd/0 -m $kmer_size -s 100M -t 2 -C -F 2 -o ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf
	jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
	# run a script to find the missing kmers and fill in a zero
	~/miniconda3/bin/python3 ~/deep_learning_microbiome/scripts/fill_in_zeros.py $file $kmer_size $data_set
    fi
    
done <  ~/deep_learning_microbiome/data/metadata/Feng_CRC_samples_only.txt 




# Jellyfish on LiverCirrhosis
# first have to combine fastq files
python ~/shattuck/metagenomic_fastq_files/LiverCirrhosis/combine_fastq_files.py
python ~/deep_learning_microbiome/scripts/jellyfish_wrapper_LiverCirrhosis.py

# check if all the kmers are outputted. If not, I need to then add in a zero
data_set='LiverCirrhosis'
kmer_size=10
while read file; do
    echo $file
    jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
    num_lines=`wc -l tmpOut.txt | cut -f1 -d' '`
    if [ $num_lines = 1049600 ]
	then
	echo 'file is ok...'
	    #cat tmpOut.txt | grep '>' | gzip > ~/deep_learning_microbiome/data/10mers_jf/${data_set}/${file}_${kmer_size}mer.gz
	else
	echo 'filling in zeros...'
	# rerun jellyfish
	dir='/pollard/shattuck0/snayfach/metagenomes/LiverCirrhosis/fastq'
	jellyfish count <(zcat ${dir}/${file}_1.fastq.gz) <(zcat ${dir}/${file}_2.fastq.gz) /dev/fd/0 -m $kmer_size -s 100M -t 2 -C -F 2 -o ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf
	jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
	# run a script to find the missing kmers and fill in a zero
	~/miniconda3/bin/python3 ~/deep_learning_microbiome/scripts/fill_in_zeros.py $file $kmer_size $data_set
    fi
    
done <  ~/deep_learning_microbiome/data/metadata/LiverCirrhosis_ids.txt



# rename the files
python rename_kmerized_files_LiverCirrhosis.py



# Jellyfish on Karlsson et al. 
python ~/deep_learning_microbiome/scripts/jellyfish_wrapper_Karlsson.py

# rerun jellyfish on Karlsson without the adapters.

# final check 
qsub ~/deep_learning_microbiome/scripts/final_check_Karlsson_no_adapter_qsub


data_set='Karlsson_2013'
kmer_size=8
while read file; do
    echo $file
    jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
    num_lines=`wc -l tmpOut.txt | cut -f1 -d' '`
    if [ $num_lines = 65792 ]
	then
	echo 'file is ok...'
	    #cat tmpOut.txt | grep '>' | gzip > ~/deep_learning_microbiome/data/10mers_jf/${data_set}/${file}_${kmer_size}mer.gz
	else
	echo 'filling in zeros...'
	# rerun jellyfish
	dir='/pollard/home/ngarud/shattuck/metagenomic_fastq_files/Karlsson_2013/fastq_files'
	jellyfish count <(zcat ${dir}/${file}_1.fastq.gz) <(zcat ${dir}/${file}_2.fastq.gz) /dev/fd/0 -m $kmer_size -s 100M -t 2 -C -F 2 -o ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf
	jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
	# run a script to find the missing kmers and fill in a zero
	~/miniconda3/bin/python3 ~/deep_learning_microbiome/scripts/fill_in_zeros.py $file $kmer_size $data_set
    fi
    
done <  ~/shattuck/metagenomic_fastq_files/Karlsson_2013/PRJEB1786_run_accessions_only.txt



# Jellyfish on Zeller et al. 




data_set='Zeller_2014'
kmer_size=10
while read file; do
    echo $file
    jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
    num_lines=`wc -l tmpOut.txt | cut -f1 -d' '`
    if [ $num_lines = 1049600 ]
	then
	echo 'file is ok...'
	    #cat tmpOut.txt | grep '>' | gzip > ~/deep_learning_microbiome/data/10mers_jf/${data_set}/${file}_${kmer_size}mer.gz
	else
	echo 'filling in zeros...'
	# rerun jellyfish
	dir='/pollard/home/ngarud/shattuck/metagenomic_fastq_files/Zeller_2014/combined_fastq_files'
	jellyfish count <(zcat ${dir}/${file}_1.fastq.gz) <(zcat ${dir}/${file}_2.fastq.gz) /dev/fd/0 -m $kmer_size -s 100M -t 2 -C -F 2 -o ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf
	jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${data_set}/${file}_${kmer_size}mer.jf > tmpOut.txt
	# run a script to find the missing kmers and fill in a zero
	~/miniconda3/bin/python3 ~/deep_learning_microbiome/scripts/fill_in_zeros.py $file $kmer_size $data_set
    fi
    
done < ~/deep_learning_microbiome/data/metadata/Zeller_2014_ids.txt

####################
# Analysis!        #
####################
# remember, we want to be using python3. 
~/miniconda3/bin/python3

# are all the kmers outputted in the same order?
# to test this, I need to read in data. 
kmer_order_jason.py

# yes, outputted in same order.
# go back and trim input files so that I just have the counts. Also gzip them. 

#################################################
# Load kmer counts using the jellyfish notation #
#################################################





#####################################################################
# June 4, 2018
# separate reads based on whether they map to human genomes or not. 
# create a bowtie index uisng Jason's code
#####################################################################

cd ~/deep_learning_microbiome/separate_reads

cut -f1 human_patric.txt | sed '1d' | sed 's/$/\.fna/g' | xargs -I{} sh -c "cat /pollard/data/microbial_genomes/patric/genomes/{}" > human_patric.fna 2> err.log
#Maybe pause a little bit to check the err.log after the command line execution finished, in case some ids in the list doesn't have genome sequence copies on server bueno.

mkdir human_patric_bt2
bowtie2-build --large-index human_patric.fna human_patric_bt2/human_patric > build_err.log
#Bowtie2 suite is needed here, and the argument "--threads" may be used for speeding up the index building.

# run bowtie2
bowtie2 --no-unal -x [bowtie2db directory/bowtie2db basename] -u [maximum reads] --very-fast-local --threads [number of threads] -q|-f [fasta input|fastq input] -1 [input reads file 1] -2 [input reads file 2]


bowtie2 --no-unal -x /pollard/home/ngarud/deep_learning_microbiome/separate_reads/human_patric_bt2/human_patric -u 10 --very-fast-local --threads 5 -f -1 /pollard/shattuck0/snayfach/metagenomes/T2D/fastq/SRR341641_1.fastq.gz -2 /pollard/shattuck0/snayfach/metagenomes/T2D/fastq/SRR341641_2.fastq.gz



#################################################
# June 11, 2018                                 #
# Implement different versions of the  model    #
#################################################

supervised_learning_model_test_Nandita.py

################################################
# July 26, 2018
# Run Katherine's code
################################################

# step 1: Modify exp_config.py to explore the functions we want and data type we want

# Step 2 make sure that in deep_learning_supervised_katherine_jf.py exp_mode = SEARCH_SUPER_MODELS. 

# now run:

~/./miniconda3/bin/python deep_learning_supervised_Katherine_jf.py >> grid_search.txt

# parse the log:
cat grid_search.txt | ~/./miniconda3/bin/python process_perf_logs.py 

# I ran the following for Liver Cirrhosis:
7mers
[1, -1]
enc_dim_key:       [ [50, 200 ]
dropout_pct_key:   [ [0, 0.1, 0.25, 0.5]

nohup nice ~/./miniconda3/bin/python deep_learning_supervised_Katherine_jf.py >> ~/deep_learning_microbiome/analysis/grid_search/LiverCirrhosis_7mers_enc_50_20_dropout.txt &

cat ~/deep_learning_microbiome/analysis/grid_search/LiverCirrhosis_7mers_enc_50_20_dropout.txt | ~/./miniconda3/bin/python process_perf_logs.py  > ~/deep_learning_microbiome/analysis/grid_search/LiverCirrhosis_7mers_enc_50_20_dropout_parsed.txt

# 8  mers for Liver Cirrhosis
nohup nice ~/./miniconda3/bin/python deep_learning_supervised_Katherine_jf.py >> ~/deep_learning_microbiome/analysis/grid_search/LiverCirrhosis_8mers.txt &

cat ~/deep_learning_microbiome/analysis/grid_search/LiverCirrhosis_8mers.txt | ~/./miniconda3/bin/python process_perf_logs.py  > ~/deep_learning_microbiome/analysis/grid_search/LiverCirrhosis_8mers_parsed.txt

# 8  mers for Karlsson
nohup nice ~/./miniconda3/bin/python deep_learning_supervised_Katherine_jf.py >> ~/deep_learning_microbiome/analysis/grid_search/Karlsson_8mers.txt &

cat ~/deep_learning_microbiome/analysis/grid_search/Karlsson_8mers.txt | ~/./miniconda3/bin/python process_perf_logs.py  > ~/deep_learning_microbiome/analysis/grid_search/Karlsson_8mers_parsed.txt


#Zeller
nohup nice ~/./miniconda3/bin/python deep_learning_supervised_Katherine_jf.py >> ~/deep_learning_microbiome/analysis/grid_search/Zeller_8mers.txt &

cat ~/deep_learning_microbiome/analysis/grid_search/Zeller_8mers.txt | ~/./miniconda3/bin/python process_perf_logs.py  > ~/deep_learning_microbiome/analysis/grid_search/Zeller_8mers_parsed.txt

# 8  mers for RA
nohup nice ~/./miniconda3/bin/python deep_learning_supervised_Katherine_jf.py >> ~/deep_learning_microbiome/analysis/grid_search/RA_8mers.txt &

cat ~/deep_learning_microbiome/analysis/grid_search/RA_8mers.txt | ~/./miniconda3/bin/python process_perf_logs.py  > ~/deep_learning_microbiome/analysis/grid_search/RA_8mers_parsed.txt


# linear models
nohup nice ~/./miniconda3/bin/python all_linear_models_katherine.py -m svm -k 10 >> ~/deep_learning_microbiome/analysis/grid_search/LiverCirrhosis_svm_10mers.txt &

nohup nice ~/./miniconda3/bin/python all_linear_models_katherine_Qin.py -m svm -k 10 >> ~/deep_learning_microbiome/analysis/grid_search/Qin_et_al_svm_10mers.txt &

nohup nice ~/./miniconda3/bin/python all_linear_models_katherine_MetaHIT.py -m svm -k 10 >> ~/deep_learning_microbiome/analysis/grid_search/MetaHIT_et_al_svm_10mers.txt &

nohup nice ~/./miniconda3/bin/python all_linear_models_katherine_RA.py -m svm -k 10 >> ~/deep_learning_microbiome/analysis/grid_search/RA_et_al_svm_10mers.txt &

nohup nice ~/./miniconda3/bin/python all_linear_models_katherine_Feng.py -m svm -k 10 >> ~/deep_learning_microbiome/analysis/grid_search/Feng_et_al_svm_10mers.txt &

nohup nice ~/./miniconda3/bin/python all_linear_models_katherine_Zeller.py -m svm -k 10 >> ~/deep_learning_microbiome/analysis/grid_search/Zeller_et_al_svm_10mers.txt &

nohup nice ~/./miniconda3/bin/python all_linear_models_katherine_Karlsson.py -m svm -k 10 >> ~/deep_learning_microbiome/analysis/grid_search/Karlsson_et_al_svm_10mers.txt &



# rerun the best models:
nohup nice ~/./miniconda3/bin/python3 linear_stats_plots.py >> ~/deep_learning_microbiome/analysis/grid_search/final_svm_search.txt &



# Informative kmers 

# what is their distribution in the raw fastq file?

dir='/pollard/home/ngarud/shattuck/metagenomic_fastq_files/Karlsson_2013/fastq_files/'
rm ~/deep_learning_microbiome/analysis/importances_karlsson_RF_10mer_frequencies.txt
echo -e "accession\tkmer\tscore\tcount_f_1\tcount_f_2\tcount_r_1\tcount_r_2" > ~/deep_learning_microbiome/analysis/importances_karlsson_RF_10mer_frequencies.txt
while read line; do
    kmer=`echo $line | cut -f1 -d' '`
    score=`echo $line | cut -f2 -d' '`
    rev_kmer=`echo $kmer | rev | tr ATGC TACG`
    while read accession; do
	count_f_1=`zcat ${dir}/${accession}_1.fastq.gz | grep $kmer | wc -l`
	count_f_2=`zcat ${dir}/${accession}_2.fastq.gz | grep $kmer | wc -l`
	count_r_1=`zcat ${dir}/${accession}_1.fastq.gz | grep $rev_kmer | wc -l`
	count_r_2=`zcat ${dir}/${accession}_2.fastq.gz | grep $rev_kmer | wc -l`
	echo -e "$accession\t$kmer\t$score\t$count_f_1\t$count_f_2\t$count_r_1\t$count_r_2" >> ~/deep_learning_microbiome/analysis/importances_karlsson_RF_10mer_frequencies.txt
	echo -e "$accession\t$kmer\t$score\t$count_f_1\t$count_f_2\t$count_r_1\t$count_r_2"
    done < ~/shattuck/metagenomic_fastq_files/Karlsson_2013/PRJEB1786_run_accessions_only.txt
done < ~/deep_learning_microbiome/analysis/importances_karlsson_linear_best_model.txt



# DO we see these weird patterns for other data sets?



dir='/pollard/home/ngarud/shattuck/metagenomic_fastq_files/Zeller_2014/fastq_files/'
rm ~/deep_learning_microbiome/analysis/importances_Zeller_trial_RF_10mer_frequencies.txt
echo -e "accession\tkmer\tscore\tcount_f_1\tcount_f_2\tcount_r_1\tcount_r_2" > ~/deep_learning_microbiome/analysis/importances_Zeller_trial_RF_10mer_frequencies.txt
while read line; do
    kmer=`echo $line | cut -f1 -d' '`
    score=`echo $line | cut -f2 -d' '`
    rev_kmer=`echo $kmer | rev | tr ATGC TACG`
    while read accession; do
	count_f_1=`zcat ${dir}/${accession}_1.fastq.gz | grep $kmer | wc -l`
	count_f_2=`zcat ${dir}/${accession}_2.fastq.gz | grep $kmer | wc -l`
	count_r_1=`zcat ${dir}/${accession}_1.fastq.gz | grep $rev_kmer | wc -l`
	count_r_2=`zcat ${dir}/${accession}_2.fastq.gz | grep $rev_kmer | wc -l`
	echo -e "$accession\t$kmer\t$score\t$count_f_1\t$count_f_2\t$count_r_1\t$count_r_2" >> ~/deep_learning_microbiome/analysis/importances_Zeller_trial_RF_10mer_frequencies.txt
	echo -e "$accession\t$kmer\t$score\t$count_f_1\t$count_f_2\t$count_r_1\t$count_r_2"
    done < ~/shattuck/metagenomic_fastq_files/Zeller_2014/PRJEB6070_WGS_run_accessions_only.txt
done < ~/deep_learning_microbiome/analysis/importances_karlsson_linear_best_model.txt


# repeat for metaHIT

dir='/pollard/shattuck0/snayfach/metagenomes/MetaHIT/fastq/'
echo -e "accession\tkmer\tscore\tcount_f_1\tcount_f_2\tcount_r_1\tcount_r_2" > ~/deep_learning_microbiome/analysis/importances_metahit_RF_10mer_frequencies.txt
while read line; do
    kmer=`echo $line | cut -f1 -d' '`
    score=`echo $line | cut -f2 -d' '`
    rev_kmer=`echo $kmer | rev | tr ATGC TACG`
    while read accession; do
	count_f_1=`zcat ${dir}/${accession}_1.fastq.gz | grep $kmer | wc -l`
	count_f_2=`zcat ${dir}/${accession}_2.fastq.gz | grep $kmer | wc -l`
	count_r_1=`zcat ${dir}/${accession}_1.fastq.gz | grep $rev_kmer | wc -l`
	count_r_2=`zcat ${dir}/${accession}_2.fastq.gz | grep $rev_kmer | wc -l`
	echo -e "$accession\t$kmer\t$score\t$count_f_1\t$count_f_2\t$count_r_1\t$count_r_2" >> ~/deep_learning_microbiome/analysis/importances_metahit_RF_10mer_frequencies.txt
	echo -e "$accession\t$kmer\t$score\t$count_f_1\t$count_f_2\t$count_r_1\t$count_r_2"
    done  < ~/deep_learning_microbiome/MetaHIT_accessions_only.txt
done < ~/deep_learning_microbiome/analysis/metahit_important_features.txt




# problems with Karlsson data:
ERR260213_1
ERR260212_2


qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_Backhed
qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_Nielsen


#################################
# August 18, 2018
# Adapter trimming
#################################

# list of adapters here: 
# Illumina
~/deep_learning_microbiome/adapters/adaptors_list.fa

#Nextera
adapter_nexteraR1.fa
adapter_nexteraR2.fa

#Truseq
adapter_truseqR1.fa
adapter_truseqR2.fa

for dataset in MetaHIT RA Karlsson_2013 Qin_et_al Zeller_2014 Feng LiverCirrhosis HMP1-2 Twins IGC Peru Nielsen LeChatelier Fiji Backhed Hadza Mongolian Raymond Yassour Ferretti; do
    nohup nice  ~/./miniconda3/bin/python grep_adapter.py -dataset_in $dataset &
done

# remove the adapters from the following data sets: HMP1-2, Karlsson_2013, and RA
dir='/pollard/home/ngarud/shattuck/metagenomic_fastq_files/Karlsson_2013/fastq_files'


skewer -m pe -t 5 -x ~/deep_learning_microbiome/adapters/adapter_truseqR1.fa -y ~/deep_learning_microbiome/adapters/adapter_truseqR2.fa ${dir}/ERR260132_1.fastq.gz ${dir}/ERR260132_2.fastq.gz -o testout.txt

skewer -m any -t 8 -k 20 -x ~/deep_learning_microbiome/adapters/adaptors_list.fa -y ~/deep_learning_microbiome/adapters/adaptors_list.fa ${dir}/ERR260132_1.fastq.gz ${dir}/ERR260132_2.fastq.gz -o testout.txt


skewer -m pe -t 8 -x ~/deep_learning_microbiome/adapters/adaptors_list.fa -y ~/deep_learning_microbiome/adapters/adaptors_list.fa ${dir}/ERR260132_1.fastq.gz ${dir}/ERR260132_2.fastq.gz -o testout.txt    

skewer -m any -k 10 -t 8 -x ~/deep_learning_microbiome/adapters/adaptors_list.fa -y ~/deep_learning_microbiome/adapters/adaptors_list.fa testout.txt-trimmed-pair1.fastq testout.txt-trimmed-pair2.fastq -o testout_2.txt    

# set up a skewer of the karlsson data

# first concatenate the adapter list because all adapters show up in Karlsson et al. 
cat ~/deep_learning_microbiome/adapters/adaptors_list.fa ~/deep_learning_microbiome/adapters/adapter_nexteraR1.fa ~/deep_learning_microbiome/adapters/adapter_truseqR1.fa > ~/deep_learning_microbiome/adapters/all_adapters_R1.fa

cat ~/deep_learning_microbiome/adapters/adaptors_list.fa ~/deep_learning_microbiome/adapters/adapter_nexteraR2.fa ~/deep_learning_microbiome/adapters/adapter_truseqR2.fa > ~/deep_learning_microbiome/adapters/all_adapters_R2.fa

cat ~/deep_learning_microbiome/adapters/adaptors_list.fa  ~/deep_learning_microbiome/adapters/adapter_truseqR1.fa > ~/deep_learning_microbiome/adapters/illumina_truseq_adapters_R1.fa

cat ~/deep_learning_microbiome/adapters/adaptors_list.fa  ~/deep_learning_microbiome/adapters/adapter_truseqR2.fa > ~/deep_learning_microbiome/adapters/illumina_truseq_adapters_R2.fa


# (manually removed a new line char from both)

# next write a qsub script to remove the karlsson adapter
qsub_remove_adapter_karlsson

# the problem with this is I cannot run skewer on QB3. Instead, run this on beuno


dir='/pollard/home/ngarud/shattuck/metagenomic_fastq_files/Karlsson_2013/fastq_files'

while read file; do
    echo $file
    skewer -m any -t 11 -k 10 -x ~/deep_learning_microbiome/adapters/all_adapters_R1.fa -y ~/deep_learning_microbiome/adapters/all_adapters_R2.fa ${dir}/${file}_1.fastq.gz ${dir}/${file}_2.fastq.gz -o ~/shattuck/metagenomic_fastq_files/Karlsson_2013/fastq_files_no_adapter/${file}
done < /pollard/home/ngarud/shattuck/metagenomic_fastq_files/Karlsson_2013/PRJEB1786_run_accessions_only.txt


# compress the resulting files:
while read file; do
    echo $file
    gzip /pollard/home/ngarud/shattuck/metagenomic_fastq_files/Karlsson_2013/fastq_files_no_adapter/${file}-trimmed-pair1.fastq
    gzip /pollard/home/ngarud/shattuck/metagenomic_fastq_files/Karlsson_2013/fastq_files_no_adapter/${file}-trimmed-pair2.fastq
done < /pollard/home/ngarud/shattuck/metagenomic_fastq_files/Karlsson_2013/PRJEB1786_run_accessions_only.txt

qsub ~/deep_learning_microbiome/scripts/qsub_compress_fastq_Karlsson

# Do the same with RA -- some weird stuff happening with that dataset

dir='/pollard/shattuck0/snayfach/metagenomes/RheumatoidArthritis/fastq'
while read file; do
    echo $file
    skewer -m any -t 11 -k 10 -x ~/deep_learning_microbiome/adapters/all_adapters_R1.fa -y ~/deep_learning_microbiome/adapters/all_adapters_R2.fa ${dir}/${file}_1.fastq.gz ${dir}/${file}_2.fastq.gz -o ~/shattuck/metagenomic_fastq_files/RA/fastq_files_no_adapter/${file}
done < ~/deep_learning_microbiome/RheumatoidArthritis_samples.txt

# do the same with Fiji 
dir="/pollard/shattuck0/snayfach/metagenomes/FIJI/fastq"
while read file; do
    echo $file
    skewer -m any -t 7 -k 10 -x ~/deep_learning_microbiome/adapters/all_adapters_R1.fa -y ~/deep_learning_microbiome/adapters/all_adapters_R2.fa ${dir}/${file}_1.fastq.gz ${dir}/${file}_2.fastq.gz -o ~/shattuck/metagenomic_fastq_files/FIJI/fastq_files_no_adapter/${file}
done < ~/shattuck/metagenomic_fastq_files/FIJI/PRJNA217052_stool_accessions_only.txt

# run the remaining on the cluster...taking forever 
qsub ~/deep_learning_microbiome/scripts/remove_adapters_Fiji
qsub ~/deep_learning_microbiome/scripts/compress_Fiji
qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_Fiji

# remove adapters from Peru data
# here I will remove the illumina adapters
dir="/pollard/shattuck0/snayfach/metagenomes/Peru/fastq"
while read file; do
    echo $file
    skewer -m any -t 4 -k 10 -x ~/deep_learning_microbiome/adapters/all_adapters_R1.fa -y ~/deep_learning_microbiome/adapters/all_adapters_R2.fa ${dir}/${file}_1.fastq.gz ${dir}/${file}_2.fastq.gz -o ~/shattuck/metagenomic_fastq_files/Peru/fastq_files_no_adapter/${file}
done < ~/shattuck/metagenomic_fastq_files/Peru/PRJNA268964_run_accession_only.txt

# compress the resulting files.
qsub ~/deep_learning_microbiome/scripts/qsub_compress_fastq_Peru

# remove adapters from HMP1-2

dir="/pollard/shattuck0/snayfach/metagenomes/Peru/fastq"
while read file; do
    echo $file
    skewer -m any -t 4 -k 10 -x ~/deep_learning_microbiome/adapters/all_adapters_R1.fa -y ~/deep_learning_microbiome/adapters/all_adapters_R2.fa ${dir}/${file}_1.fastq.gz ${dir}/${file}_2.fastq.gz -o ~/shattuck/metagenomic_fastq_files/Peru/fastq_files_no_adapter/${file}
done < ~/shattuck/metagenomic_fastq_files/Peru/PRJNA268964_run_accession_only.txt


# remove adapters for HMP1-2
qsub ~/deep_learning_microbiome/scripts/remove_adapters_HMP1-2
# kmerize HMP1-2
qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_HMP1-2


# kmerize IGC
qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_IGC
qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_IGC_paired

# kmerize Nielsen
qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_Nielsen

# kmerize LeChatelier
qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_LeChatelier

# remove adapter and kmerize twins
qsub ~/deep_learning_microbiome/scripts/remove_adapters_Twins

qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_Twins

# compress and kmerize RA
qsub ~/deep_learning_microbiome/scripts/compress_jellyfish_RA
qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_RA

# remove adapter and kmerize Hadza
qsub ~/deep_learning_microbiome/scripts/remove_adapters_Hadza

qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_Hadza

# remove adapter for Mongolian
qsub ~/deep_learning_microbiome/scripts/remove_adapters_Mongolian
qsub ~/deep_learning_microbiome/scripts/remove_adapters_Mongolian_rerun
# some adapters didn't get removed
while read accession; do
    if [ ! -f ~/shattuck/metagenomic_fastq_files/Mongolian/fastq_files_no_adapter/${accession}-trimmed-pair1.fastq.gz ]; then
	echo $accession >> /pollard/home/ngarud/shattuck/metagenomic_fastq_files/Mongolian/PRJNA328899_run_accessions_rerun.txt
    fi
done < /pollard/home/ngarud/shattuck/metagenomic_fastq_files/Mongolian/PRJNA328899_run_accessions_only.txt


qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_Mongolian

# remove adapter for Raymond
qsub ~/deep_learning_microbiome/scripts/remove_adapters_Raymond
qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_Raymond

# remove adapter for Yassour
qsub ~/deep_learning_microbiome/scripts/remove_adapters_Yassour
qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_Yassour

# kmerize Ferretri
qsub ~/deep_learning_microbiome/scripts/qsub_jellyfish_Ferretti

# final checks (fill in zeros if a kmer is absent)
qsub ~/deep_learning_microbiome/scripts/final_check_Karlsson_no_adapter_qsub
qsub ~/deep_learning_microbiome/scripts/final_check_Backhed_no_adapter_qsub


##############################################################
# run jellyfish linearly -- cluster doesn't have enough ram  #
##############################################################

#RA
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_RA_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_RA_linear_2nd_run.log &

nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_RA_linear_rerun.sh > ~/deep_learning_microbiome/analysis/jellyfish_RA_linear_rerun.log &

#IGC
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_IGC_paired_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_IGC_paired_linear.log &

nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_IGC_single_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_IGC_single_linear.log &

nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_IGC_paired_linear_rerun.sh > ~/deep_learning_microbiome/analysis/jellyfish_IGC_paired_linear_rerun.log &

python combine_kmerized_files_IGC.py

#Peru
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Peru_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_Peru_linear.log &

nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Peru_linear_rerun.sh > ~/deep_learning_microbiome/analysis/jellyfish_Peru_linear_rerun.log &

#Hadza -- TO RUN
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Hadza_linear.sh >> ~/deep_learning_microbiome/analysis/jellyfish_Hadza_linear.log &

#HMP -- TO RUN. Also, running this with 6 threads because it's a large dataset. 
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_HMP1-2_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_HMP1-2_linear.log &

nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_HMP1-2_linear_rerun.sh > ~/deep_learning_microbiome/analysis/jellyfish_HMP1-2_linear_rerun.log &

#Backhed
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Backhed_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_Backhed_linear_Sept.log &

nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Backhed_linear_rerun.sh > ~/deep_learning_microbiome/analysis/jellyfish_Backhed_linear.log &

# Nielsen
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Nielsen_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_Nielsen_linear.log &

# LeChatelier
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_LeChatelier_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_LeChatelier_linear.log &

#Twins
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Twins_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_Twins_linear_Sept.log &

nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Twins_linear_rerun.sh > ~/deep_learning_microbiome/analysis/jellyfish_Twins_linear_rerun.log &

# Raymond
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Raymond_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_Raymond_linear.log &

# Mongolian
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Mongolian_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_Mongolian_linear.log &

nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Mongolian_linear_rerun.sh > ~/deep_learning_microbiome/analysis/jellyfish_Mongolian_linear_rerun.log &

#Ferretti
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Ferretti_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_Ferretti_linear.log &

nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Ferretti_linear_rerun.sh > ~/deep_learning_microbiome/analysis/jellyfish_Ferretti_linear_rerun.log &

# Fiji
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Fiji_linear_rerun.sh > ~/deep_learning_microbiome/analysis/jellyfish_Fiji_linear.log &

python combine_kmerized_files_Fiji.py

# Yassour
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Yassour_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_Yassour_linear.log &

nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Yassour_linear_rerun.sh > ~/deep_learning_microbiome/analysis/jellyfish_Yassour_linear_rerun.log &

# Zeller
nohup nice bash ~/deep_learning_microbiome/scripts/jellyfish_Zeller_linear.sh > ~/deep_learning_microbiome/analysis/jellyfish_Zeller_linear.log &

# check if kmer counts are nonzero

dataset=HMP1-2
for kmer_size in 5 6 7 8 10; do
    echo $kmer_size >> ~/deep_learning_microbiome/analysis/${dataset}_kmer_count.txt
    while read file; do
	file=~/deep_learning_microbiome/data/8mers_jf/HMP1-2/${file}_${kemr_size}mer.gz
	python ~/deep_learning_microbiome/scripts/count_kmers.py $file >> ~/deep_learning_microbiome/analysis/${dataset}_kmer_count.txt
    done < ~/deep_learning_microbiome/data/metadata/HMP1-2_samples_only.txt
done


nohup nice bash count_kmers_HMP.sh &
nohup nice bash count_kmers_RA.sh &
nohup nice bash count_kmers_IGC.sh &
nohup nice bash count_kmers_Peru.sh &
nohup nice bash count_kmers_Fiji.sh &
nohup nice bash count_kmers_Backhed.sh &
nohup nice bash count_kmers_Zeller.sh &
nohup nice bash count_kmers_Raymond.sh &
nohup nice bash count_kmers_Yassour.sh &
nohup nice bash count_kmers_Ferretti.sh &
nohup nice bash count_kmers_Twins.sh & 
nohup nice bash count_kmers_Mongolian.sh & 



dataset=HMP1-2
for kmer_size in 5 6 7 8 10; do
    while read file; do
	file_path=~/deep_learning_microbiome/data/${kmer_size}mers_jf/${dataset}/${file}_${kmer_size}mer.gz
	count_zero=`zcat $file_path | grep '>0' | wc -l `
	if [ "$count_zero" -gt 0 ]; then
	    echo $file  >> ~/deep_learning_microbiome/analysis/${dataset}_${kmer_size}_rerun.txt
	fi
    done < ~/deep_learning_microbiome/data/metadata/HMP1-2_samples_only.txt
done
