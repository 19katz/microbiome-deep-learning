{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg') # this suppresses the console for plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import numpy as np\n",
    "from numpy import random, array\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path\n",
    "import pylab\n",
    "import importlib\n",
    "import imp\n",
    "from importlib import reload\n",
    "import gzip\n",
    "import ntpath\n",
    "from Bio import SeqIO\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "from scipy import interp\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize, label_binarize, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import History, TensorBoard\n",
    "from keras import backend as K\n",
    "backend = K.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our private scripts\n",
    "import load_kmer_cnts_jf\n",
    "import load_spAB\n",
    "import deep_learning_models\n",
    "import plotting_utils\n",
    "import stats_utils_AEB_110718\n",
    "import stats_utils\n",
    "import config_file\n",
    "\n",
    "species_directory = config_file.species_directory\n",
    "data_directory = config_file.data_directory\n",
    "analysis_directory = config_file.analysis_directory  \n",
    "scripts_directory = config_file.scripts_directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data_set, kmer_size, norm_input, encoding_dim, encoded_activation, input_dropout_pct, dropout_pct, num_epochs, batch_size, n_splits, n_repeats, compute_informative_features, plot_iteration, graph_dir, outFile):\n",
    "    \n",
    "    # format strings for outputting the paramters associated with this run:\n",
    "    summary_string, plotting_string= stats_utils.format_input_parameters_printing(data_set, kmer_size, norm_input, encoding_dim, encoded_activation,input_dropout_pct,dropout_pct,num_epochs,batch_size,n_splits,n_repeats,compute_informative_features,plot_iteration)\n",
    "\n",
    "    outFile_header='data_set\\tkmer_size\\tnorm_input\\tencoding_dim\\tencoded_activation\\tinput_dropout_pct\\tdropout_pct\\tnum_epochs\\tbatch_size\\tn_splits\\tn_repeats\\t'\n",
    "\n",
    "    #################\n",
    "    # Load the data # \n",
    "    #################\n",
    "    print('Loading data...')\n",
    "\n",
    "    data_normalized, labels, rskf = load_spAB.load_single_disease(data_set, n_splits, n_repeats, precomputed_kfolds=False)\n",
    "\n",
    "    # rskf = repeated stratified k fold. This contains all the kfold-by-iteration combos. \n",
    "\n",
    "\n",
    "    ###################################################\n",
    "    # iterate through the data kfolds and iterations #\n",
    "    ###################################################\n",
    "\n",
    "    # Create a dictionary to store the metrics of each fold \n",
    "    aggregated_statistics={} # key=n_repeat, values= dictionary with stats\n",
    "\n",
    "    for n_repeat in range(0,len(rskf[0])):\n",
    "        \n",
    "        print('Iteration %s...' %n_repeat)\n",
    "        \n",
    "        aggregated_statistics[n_repeat] = {}\n",
    "        \n",
    "        train_idx = rskf[0][n_repeat]\n",
    "        test_idx = rskf[1][n_repeat]\n",
    "        x_train, y_train = data_normalized[train_idx], labels[train_idx]\n",
    "        x_test, y_test = data_normalized[test_idx], labels[test_idx]\n",
    "    \n",
    "        #standardize the data, mean=0, std=1\n",
    "        if norm_input:\n",
    "            x_train, x_test= stats_utils.standardize_data(x_train, x_test)\n",
    "    \n",
    "        ###########################################\n",
    "        # set up a model (supervised learning)    #\n",
    "        ###########################################\n",
    "        # note that the model has to be instantiated each time a new fold is started otherwise the weights will not start from scratch. \n",
    "    \n",
    "        input_dim=len(data_normalized[0]) # this is the number of input kmers\n",
    "\n",
    "        model=deep_learning_models.create_supervised_model(input_dim, encoding_dim, encoded_activation,input_dropout_pct, dropout_pct)\n",
    "    \n",
    "        #weightFile = os.environ['HOME'] + '/deep_learning_microbiome/data/weights.txt'\n",
    "       \n",
    "        ##################################################\n",
    "        # Fit the model with the train data of this fold #\n",
    "        ##################################################\n",
    "        history = History()\n",
    "        # history is a dictionary. To get the keys, type print(history.history.keys())\n",
    "        \n",
    "        model.fit(x_train, y_train, \n",
    "                  epochs=num_epochs, \n",
    "                  batch_size=len(x_train), \n",
    "                  shuffle=True,\n",
    "                  validation_data=(x_test, y_test),\n",
    "                  verbose=0,\n",
    "                  callbacks=[history])\n",
    "    \n",
    "        # predict using the held out data\n",
    "        y_pred=model.predict(x_test)\n",
    "        \n",
    "        # save the weights of this model. TODO \n",
    "    \n",
    "        ################################################################\n",
    "        # Compute summary statistics                                   #\n",
    "        ################################################################\n",
    "        # Store the results of this fold in aggregated_statistics\n",
    "        aggregated_statistics = stats_utils.compute_summary_statistics(y_test, y_pred, history, aggregated_statistics, n_repeat)\n",
    "\n",
    "        # could  plot everything (roc, accuracy vs epoch, loss vs epoch, confusion matrix, precision recall) for each fold, but this will produce a lot of graphs. \n",
    "        if compute_informative_features:\n",
    "            shap_values, shap_values_summed = stats_utils.compute_shap_values_deeplearning(input_dim, model, x_test)\n",
    "            aggregated_statistics[n_repeat]['shap_values_summed']=shap_values_summed\n",
    "            aggregated_statistics[n_repeat]['shap_values']=shap_values\n",
    "\n",
    "        # also plot:\n",
    "        #shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
    "        #shap.summary_plot(shap_values, X)\n",
    "\n",
    "    ##############################################\n",
    "    # aggregate the results from all the k-folds #\n",
    "    # Print and Plot                             #\n",
    "    ##############################################\n",
    "    print('Aggregating statistics across iterations and printing/plotting...')\n",
    "\n",
    "    stats_utils.aggregate_statistics_across_folds(aggregated_statistics, rskf, n_splits, outFile, summary_string, plotting_string, outFile_header)\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # Aggregate shap: #\n",
    "    ###################\n",
    "\n",
    "    if compute_informative_features: \n",
    "        print('Computing informative features with Shap...')\n",
    "        stats_utils.aggregate_shap(aggregated_statistics, rskf)\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    # TSNE visualization                #\n",
    "    # Annamarie                         #\n",
    "    # find the weights of the best fold #\n",
    "    #####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "# parser for the config dict #\n",
    "##############################\n",
    "def parse_config_and_run(config_dict, outFile):\n",
    "    data_sets=config_dict['data_set']\n",
    "    kmer_sizes=config_dict['kmer_size']\n",
    "    norm_inputs=config_dict['norm_input']\n",
    "    encoding_dims=config_dict['encoding_dim']\n",
    "    encoded_activations=config_dict['encoded_activation']\n",
    "    input_dropout_pcts=config_dict['input_dropout_pct']\n",
    "    dropout_pcts=config_dict['dropout_pct'] \n",
    "    num_epochss=config_dict['num_epochs']\n",
    "    batch_sizes=config_dict['batch_size']\n",
    "    n_splitss=config_dict['n_splits']\n",
    "    n_repeatss=config_dict['n_repeats']\n",
    "    compute_informative_featuress=config_dict['compute_informative_features']\n",
    "    plot_iterations=config_dict['plot_iteration'] \n",
    "    graph_dirs=config_dict['graph_dir'] \n",
    "\n",
    "    for data_set in data_sets:\n",
    "        for kmer_size in kmer_sizes:\n",
    "            for norm_input in norm_inputs:\n",
    "                for encoding_dim in encoding_dims:\n",
    "                    for encoded_activation in encoded_activations:\n",
    "                        for input_dropout_pct in input_dropout_pcts:\n",
    "                            for dropout_pct in dropout_pcts:\n",
    "                                for num_epochs in num_epochss:\n",
    "                                    for batch_size in batch_sizes:\n",
    "                                        for n_splits in n_splitss:\n",
    "                                            for n_repeats in n_repeatss:\n",
    "                                                for compute_informative_features in compute_informative_featuress:\n",
    "                                                    for plot_iteration in plot_iterations:\n",
    "                                                        for graph_dir in graph_dirs:\n",
    "                                                        \n",
    "                                                            run_model(data_set, \n",
    "                                                                      kmer_size,\n",
    "                                                                      norm_input,\n",
    "                                                                      encoding_dim,\n",
    "                                                                      encoded_activation,\n",
    "                                                                      input_dropout_pct,\n",
    "                                                                      dropout_pct,\n",
    "                                                                      num_epochs,\n",
    "                                                                      batch_size,\n",
    "                                                                      n_splits,\n",
    "                                                                      n_repeats,\n",
    "                                                                      compute_informative_features,\n",
    "                                                                      plot_iteration,\n",
    "                                                                      graph_dir, \n",
    "                                                                      outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef run_model(data_set, norm_input, encoding_dim, encoded_activation, input_dropout_pct, dropout_pct, num_epochs, batch_size, n_splits, n_repeats, compute_informative_features, plot_iteration, graph_dir, outFile):\\n    \\n    # format strings for outputting the paramters associated with this run:\\n    summary_string, plotting_string= stats_utils_AEB_110718.format_input_parameters_printing(data_set, norm_input=True, encoding_dim=8, encoded_activation=\\'sigmoid\\',input_dropout_pct=0,dropout_pct=0,num_epochs=5,batch_size=16,n_splits=10,n_repeats=5,compute_informative_features=False)\\n\\n    outFile_header=\\'data_set\\tnorm_input\\tencoding_dim\\tencoded_activation\\tinput_dropout_pct\\tdropout_pct\\tnum_epochs\\tbatch_size\\tn_splits\\tn_repeats\\t\\'\\n\\n    #################\\n    # Load the data # \\n    #################\\n    print(\\'Loading data...\\')\\n\\n    data_normalized, labels, rskf = load_spAB.load_single_disease(data_set, n_splits, n_repeats, precomputed_kfolds=False)\\n\\n    # rskf = repeated stratified k fold. This contains all the kfold-by-iteration combos. \\n\\n\\n    ###################################################\\n    # iterate through the data kfolds and iterations #\\n    ###################################################\\n\\n    # Create a dictionary to store the metrics of each fold \\n    aggregated_statistics={} # key=n_repeat, values= dictionary with stats\\n\\n    for n_repeat in range(0,len(rskf[0])):\\n        \\n        print(\\'Iteration %s...\\' %n_repeat)\\n        \\n        aggregated_statistics[n_repeat] = {}\\n        \\n        train_idx = rskf[0][n_repeat]\\n        test_idx = rskf[1][n_repeat]\\n        x_train, y_train = data_normalized[train_idx], labels[train_idx]\\n        x_test, y_test = data_normalized[test_idx], labels[test_idx]\\n    \\n        #standardize the data, mean=0, std=1\\n        if norm_input:\\n            x_train, x_test= stats_utils.standardize_data(x_train, x_test)\\n    \\n        ###########################################\\n        # set up a model (supervised learning)    #\\n        ###########################################\\n        # note that the model has to be instantiated each time a new fold is started otherwise the weights will not start from scratch. \\n    \\n        input_dim=len(data_normalized[0]) # this is the number of input kmers\\n\\n        model=deep_learning_models.create_supervised_model(input_dim, encoding_dim, encoded_activation,input_dropout_pct, dropout_pct)\\n    \\n        #weightFile = os.environ[\\'HOME\\'] + \\'/deep_learning_microbiome/data/weights.txt\\'\\n       \\n        ##################################################\\n        # Fit the model with the train data of this fold #\\n        ##################################################\\n        history = History()\\n        # history is a dictionary. To get the keys, type print(history.history.keys())\\n        \\n        model.fit(x_train, y_train, \\n                  epochs=num_epochs, \\n                  batch_size=batch_size, \\n                  shuffle=True,\\n                  validation_data=(x_test, y_test),\\n                  verbose=0,\\n                  callbacks=[history])\\n    \\n        # predict using the held out data\\n        y_pred=model.predict(x_test)\\n        \\n        # save the weights of this model. TODO \\n    \\n        ################################################################\\n        # Compute summary statistics                                   #\\n        ################################################################\\n        # Store the results of this fold in aggregated_statistics\\n        aggregated_statistics = stats_utils.compute_summary_statistics(y_test, y_pred, history, aggregated_statistics, n_repeat)\\n\\n        # could  plot everything (roc, accuracy vs epoch, loss vs epoch, confusion matrix, precision recall) for each fold, but this will produce a lot of graphs. \\n        if compute_informative_features:\\n            shap_values, shap_values_summed = stats_utils.compute_shap_values_deeplearning(input_dim, model, x_test)\\n            aggregated_statistics[n_repeat][\\'shap_values_summed\\']=shap_values_summed\\n            aggregated_statistics[n_repeat][\\'shap_values\\']=shap_values\\n\\n        # also plot:\\n        #shap.summary_plot(shap_values, X, plot_type=\"bar\")\\n        #shap.summary_plot(shap_values, X)\\n\\n    ##############################################\\n    # aggregate the results from all the k-folds #\\n    # Print and Plot                             #\\n    ##############################################\\n    print(\\'Aggregating statistics across iterations and printing/plotting...\\')\\n\\n    stats_utils.aggregate_statistics_across_folds(aggregated_statistics, rskf, n_splits, outFile, summary_string, plotting_string, outFile_header)\\n\\n\\n    ###################\\n    # Aggregate shap: #\\n    ###################\\n\\n    if compute_informative_features: \\n        print(\\'Computing informative features with Shap...\\')\\n        stats_utils.aggregate_shap(aggregated_statistics, rskf)\\n\\n\\n    #####################################\\n    # TSNE visualization                #\\n    # Annamarie                         #\\n    # find the weights of the best fold #\\n    #####################################\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def run_model(data_set, norm_input, encoding_dim, encoded_activation, input_dropout_pct, dropout_pct, num_epochs, batch_size, n_splits, n_repeats, compute_informative_features, plot_iteration, graph_dir, outFile):\n",
    "    \n",
    "    # format strings for outputting the paramters associated with this run:\n",
    "    summary_string, plotting_string= stats_utils_AEB_110718.format_input_parameters_printing(data_set, norm_input=True, encoding_dim=8, encoded_activation='sigmoid',input_dropout_pct=0,dropout_pct=0,num_epochs=5,batch_size=16,n_splits=10,n_repeats=5,compute_informative_features=False)\n",
    "\n",
    "    outFile_header='data_set\\tnorm_input\\tencoding_dim\\tencoded_activation\\tinput_dropout_pct\\tdropout_pct\\tnum_epochs\\tbatch_size\\tn_splits\\tn_repeats\\t'\n",
    "\n",
    "    #################\n",
    "    # Load the data # \n",
    "    #################\n",
    "    print('Loading data...')\n",
    "\n",
    "    data_normalized, labels, rskf = load_spAB.load_single_disease(data_set, n_splits, n_repeats, precomputed_kfolds=False)\n",
    "\n",
    "    # rskf = repeated stratified k fold. This contains all the kfold-by-iteration combos. \n",
    "\n",
    "\n",
    "    ###################################################\n",
    "    # iterate through the data kfolds and iterations #\n",
    "    ###################################################\n",
    "\n",
    "    # Create a dictionary to store the metrics of each fold \n",
    "    aggregated_statistics={} # key=n_repeat, values= dictionary with stats\n",
    "\n",
    "    for n_repeat in range(0,len(rskf[0])):\n",
    "        \n",
    "        print('Iteration %s...' %n_repeat)\n",
    "        \n",
    "        aggregated_statistics[n_repeat] = {}\n",
    "        \n",
    "        train_idx = rskf[0][n_repeat]\n",
    "        test_idx = rskf[1][n_repeat]\n",
    "        x_train, y_train = data_normalized[train_idx], labels[train_idx]\n",
    "        x_test, y_test = data_normalized[test_idx], labels[test_idx]\n",
    "    \n",
    "        #standardize the data, mean=0, std=1\n",
    "        if norm_input:\n",
    "            x_train, x_test= stats_utils.standardize_data(x_train, x_test)\n",
    "    \n",
    "        ###########################################\n",
    "        # set up a model (supervised learning)    #\n",
    "        ###########################################\n",
    "        # note that the model has to be instantiated each time a new fold is started otherwise the weights will not start from scratch. \n",
    "    \n",
    "        input_dim=len(data_normalized[0]) # this is the number of input kmers\n",
    "\n",
    "        model=deep_learning_models.create_supervised_model(input_dim, encoding_dim, encoded_activation,input_dropout_pct, dropout_pct)\n",
    "    \n",
    "        #weightFile = os.environ['HOME'] + '/deep_learning_microbiome/data/weights.txt'\n",
    "       \n",
    "        ##################################################\n",
    "        # Fit the model with the train data of this fold #\n",
    "        ##################################################\n",
    "        history = History()\n",
    "        # history is a dictionary. To get the keys, type print(history.history.keys())\n",
    "        \n",
    "        model.fit(x_train, y_train, \n",
    "                  epochs=num_epochs, \n",
    "                  batch_size=batch_size, \n",
    "                  shuffle=True,\n",
    "                  validation_data=(x_test, y_test),\n",
    "                  verbose=0,\n",
    "                  callbacks=[history])\n",
    "    \n",
    "        # predict using the held out data\n",
    "        y_pred=model.predict(x_test)\n",
    "        \n",
    "        # save the weights of this model. TODO \n",
    "    \n",
    "        ################################################################\n",
    "        # Compute summary statistics                                   #\n",
    "        ################################################################\n",
    "        # Store the results of this fold in aggregated_statistics\n",
    "        aggregated_statistics = stats_utils.compute_summary_statistics(y_test, y_pred, history, aggregated_statistics, n_repeat)\n",
    "\n",
    "        # could  plot everything (roc, accuracy vs epoch, loss vs epoch, confusion matrix, precision recall) for each fold, but this will produce a lot of graphs. \n",
    "        if compute_informative_features:\n",
    "            shap_values, shap_values_summed = stats_utils.compute_shap_values_deeplearning(input_dim, model, x_test)\n",
    "            aggregated_statistics[n_repeat]['shap_values_summed']=shap_values_summed\n",
    "            aggregated_statistics[n_repeat]['shap_values']=shap_values\n",
    "\n",
    "        # also plot:\n",
    "        #shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
    "        #shap.summary_plot(shap_values, X)\n",
    "\n",
    "    ##############################################\n",
    "    # aggregate the results from all the k-folds #\n",
    "    # Print and Plot                             #\n",
    "    ##############################################\n",
    "    print('Aggregating statistics across iterations and printing/plotting...')\n",
    "\n",
    "    stats_utils.aggregate_statistics_across_folds(aggregated_statistics, rskf, n_splits, outFile, summary_string, plotting_string, outFile_header)\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # Aggregate shap: #\n",
    "    ###################\n",
    "\n",
    "    if compute_informative_features: \n",
    "        print('Computing informative features with Shap...')\n",
    "        stats_utils.aggregate_shap(aggregated_statistics, rskf)\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    # TSNE visualization                #\n",
    "    # Annamarie                         #\n",
    "    # find the weights of the best fold #\n",
    "    #####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n##############################\\n# parser for the config dict #\\n##############################\\ndef parse_config_and_run(config_dict, outFile):\\n    data_sets=config_dict['data_set']\\n    #kmer_sizes=config_dict['kmer_size']\\n    norm_inputs=config_dict['norm_input']\\n    encoding_dims=config_dict['encoding_dim']\\n    encoded_activations=config_dict['encoded_activation']\\n    input_dropout_pcts=config_dict['input_dropout_pct']\\n    dropout_pcts=config_dict['dropout_pct'] \\n    num_epochss=config_dict['num_epochs']\\n    batch_sizes=config_dict['batch_size']\\n    n_splitss=config_dict['n_splits']\\n    n_repeatss=config_dict['n_repeats']\\n    compute_informative_featuress=config_dict['compute_informative_features']\\n    plot_iterations=config_dict['plot_iteration'] \\n    graph_dirs=config_dict['graph_dir'] \\n\\n    for data_set in data_sets:\\n        #for kmer_size in kmer_sizes:\\n        for norm_input in norm_inputs:\\n            for encoding_dim in encoding_dims:\\n                for encoded_activation in encoded_activations:\\n                    for input_dropout_pct in input_dropout_pcts:\\n                        for dropout_pct in dropout_pcts:\\n                            for num_epochs in num_epochss:\\n                                for batch_size in batch_sizes:\\n                                    for n_splits in n_splitss:\\n                                        for n_repeats in n_repeatss:\\n                                            for compute_informative_features in compute_informative_featuress:\\n                                                for plot_iteration in plot_iterations:\\n                                                    for graph_dir in graph_dirs:\\n                                                    \\n                                                          run_model(data_set, \\n                                                                  #kmer_size,\\n                                                                  norm_input,\\n                                                                  encoding_dim,\\n                                                                  encoded_activation,\\n                                                                  input_dropout_pct,\\n                                                                  dropout_pct,\\n                                                                  num_epochs,\\n                                                                  batch_size,\\n                                                                  n_splits,\\n                                                                  n_repeats,\\n                                                                  compute_informative_features,\\n                                                                  plot_iteration,\\n                                                                  graph_dir, \\n                                                                  outFile)\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "##############################\n",
    "# parser for the config dict #\n",
    "##############################\n",
    "def parse_config_and_run(config_dict, outFile):\n",
    "    data_sets=config_dict['data_set']\n",
    "    #kmer_sizes=config_dict['kmer_size']\n",
    "    norm_inputs=config_dict['norm_input']\n",
    "    encoding_dims=config_dict['encoding_dim']\n",
    "    encoded_activations=config_dict['encoded_activation']\n",
    "    input_dropout_pcts=config_dict['input_dropout_pct']\n",
    "    dropout_pcts=config_dict['dropout_pct'] \n",
    "    num_epochss=config_dict['num_epochs']\n",
    "    batch_sizes=config_dict['batch_size']\n",
    "    n_splitss=config_dict['n_splits']\n",
    "    n_repeatss=config_dict['n_repeats']\n",
    "    compute_informative_featuress=config_dict['compute_informative_features']\n",
    "    plot_iterations=config_dict['plot_iteration'] \n",
    "    graph_dirs=config_dict['graph_dir'] \n",
    "\n",
    "    for data_set in data_sets:\n",
    "        #for kmer_size in kmer_sizes:\n",
    "        for norm_input in norm_inputs:\n",
    "            for encoding_dim in encoding_dims:\n",
    "                for encoded_activation in encoded_activations:\n",
    "                    for input_dropout_pct in input_dropout_pcts:\n",
    "                        for dropout_pct in dropout_pcts:\n",
    "                            for num_epochs in num_epochss:\n",
    "                                for batch_size in batch_sizes:\n",
    "                                    for n_splits in n_splitss:\n",
    "                                        for n_repeats in n_repeatss:\n",
    "                                            for compute_informative_features in compute_informative_featuress:\n",
    "                                                for plot_iteration in plot_iterations:\n",
    "                                                    for graph_dir in graph_dirs:\n",
    "                                                    \n",
    "                                                          run_model(data_set, \n",
    "                                                                  #kmer_size,\n",
    "                                                                  norm_input,\n",
    "                                                                  encoding_dim,\n",
    "                                                                  encoded_activation,\n",
    "                                                                  input_dropout_pct,\n",
    "                                                                  dropout_pct,\n",
    "                                                                  num_epochs,\n",
    "                                                                  batch_size,\n",
    "                                                                  n_splits,\n",
    "                                                                  n_repeats,\n",
    "                                                                  compute_informative_features,\n",
    "                                                                  plot_iteration,\n",
    "                                                                  graph_dir, \n",
    "                                                                  outFile)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n###########\\n# Main    #\\n###########\\n\\nif __name__ == \\'__main__\\':\\n    \\n    # read in command-line arguments\\n    parser = argparse.ArgumentParser(description= \"Program to run deep learning models on kmer datasets\")\\n    parser.add_argument(\\'-outFile\\', type = str, default = \\'summary_statistics.txt\\', help = \"OutFile for saving summary statistics\")\\n    parser.add_argument(\\'-configFile\\', type = str, default = \\'none\\', help = \"Config file for running the code\")\\n\\n    arg_vals = parser.parse_args()\\n    outFile = arg_vals.outFile\\n    configFile=arg_vals.configFile\\n\\n    # Parse the config file and run the code!\\n    if configFile==\\'none\\':\\n        config_dict=config_file.config\\n    else:\\n\\n        with open(configFile, \\'rb\\') as fp:\\n            config_file = imp.load_module(\\n                configFile, fp, configFile,\\n                (\\'.py\\', \\'rb\\', imp.PY_SOURCE)\\n            )\\n\\n        config_dict=config_file.config\\n            \\n\\n    parse_config_and_run(config_dict, outFile)\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "###########\n",
    "# Main    #\n",
    "###########\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # read in command-line arguments\n",
    "    parser = argparse.ArgumentParser(description= \"Program to run deep learning models on kmer datasets\")\n",
    "    parser.add_argument('-outFile', type = str, default = 'summary_statistics.txt', help = \"OutFile for saving summary statistics\")\n",
    "    parser.add_argument('-configFile', type = str, default = 'none', help = \"Config file for running the code\")\n",
    "\n",
    "    arg_vals = parser.parse_args()\n",
    "    outFile = arg_vals.outFile\n",
    "    configFile=arg_vals.configFile\n",
    "\n",
    "    # Parse the config file and run the code!\n",
    "    if configFile=='none':\n",
    "        config_dict=config_file.config\n",
    "    else:\n",
    "\n",
    "        with open(configFile, 'rb') as fp:\n",
    "            config_file = imp.load_module(\n",
    "                configFile, fp, configFile,\n",
    "                ('.py', 'rb', imp.PY_SOURCE)\n",
    "            )\n",
    "\n",
    "        config_dict=config_file.config\n",
    "            \n",
    "\n",
    "    parse_config_and_run(config_dict, outFile)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets_to_use = [\n",
    "    [['MetaHIT'], ['MetaHIT']],\n",
    "    [['Qin_et_al'], ['Qin_et_al']],\n",
    "    [['Zeller_2014'], ['Zeller_2014']],\n",
    "    [['LiverCirrhosis'], ['LiverCirrhosis']]\n",
    "   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaHIT\n",
      "LOADED DATASET MetaHIT: 110 SAMPLES\n",
      "Qin_et_al\n",
      "LOADED DATASET Qin_et_al: 271 SAMPLES\n",
      "Zeller_2014\n",
      "LOADED DATASET Zeller_2014: 121 SAMPLES\n",
      "LiverCirrhosis\n",
      "LOADED DATASET LiverCirrhosis: 232 SAMPLES\n"
     ]
    }
   ],
   "source": [
    "for data_set in data_sets_to_use:\n",
    "    data_set = data_set[0]\n",
    "    species_cnts, labelz, feats = load_spAB.load_species(data_set)\n",
    "    print(\"LOADED DATASET \" + str(data_set[0]) + \": \" + str(len(species_cnts)) + \" SAMPLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters being tested:\n",
      "MetaHIT\n",
      "7\n",
      "Normalize input? True\n",
      "Encoding dim: 8\n",
      "Encoded activation: sigmoid\n",
      "Input dropout percent: 0\n",
      "Dropout percent: 0\n",
      "Num epochs: 400\n",
      "Batch size: 16\n",
      "n_splits (k-folds): 10\n",
      "n_repeats (iterations): 5\n",
      "Compute infromative features with Shap? False\n",
      "Plots for each iteration? False\n",
      "\n",
      "Loading data...\n",
      "MetaHIT\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/pollard/home/abustion/deep_learning_microbiome/data/precomputed_kfolds/MetaHIT_single_disease.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-04d0436123de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#for data_set in data_sets_to_use:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparse_config_and_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutFile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"summary_statistics_11718.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-39b2b114aac7>\u001b[0m in \u001b[0;36mparse_config_and_run\u001b[0;34m(config_dict, outFile)\u001b[0m\n\u001b[1;32m    159\u001b[0m                                                                       \u001b[0mplot_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                                                                       \u001b[0mgraph_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                                                                       outFile)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-39b2b114aac7>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(data_set, kmer_size, norm_input, encoding_dim, encoded_activation, input_dropout_pct, dropout_pct, num_epochs, batch_size, n_splits, n_repeats, compute_informative_features, plot_iteration, graph_dir, outFile)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdata_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrskf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_spAB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_single_disease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecomputed_kfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# rskf = repeated stratified k fold. This contains all the kfold-by-iteration combos.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep_learning_microbiome/scripts/load_spAB.py\u001b[0m in \u001b[0;36mload_single_disease\u001b[0;34m(data_set, n_splits, n_repeats, precomputed_kfolds)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%sprecomputed_kfolds'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_indexs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_indexs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"%s/%s_single_disease.p\"\u001b[0m  \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/pollard/home/abustion/deep_learning_microbiome/data/precomputed_kfolds/MetaHIT_single_disease.p'"
     ]
    }
   ],
   "source": [
    "config_dict=config_file.config\n",
    "#for data_set in data_sets_to_use:\n",
    "parse_config_and_run(config_dict, outFile=\"summary_statistics_11718.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
