{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg') # this suppresses the console for plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import numpy as np\n",
    "from numpy import random, array\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path\n",
    "import pylab\n",
    "import importlib\n",
    "import imp\n",
    "from importlib import reload\n",
    "import gzip\n",
    "import ntpath\n",
    "from Bio import SeqIO\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "from scipy import interp\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize, label_binarize, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import History, TensorBoard\n",
    "from keras import backend as K\n",
    "backend = K.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our private scripts\n",
    "import load_spAB\n",
    "import deep_learning_models\n",
    "import plotting_utils_AEB\n",
    "import stats_utils_AEB_110718\n",
    "import config_file_AEB\n",
    "\n",
    "species_directory = config_file_AEB.species_directory\n",
    "data_directory = config_file_AEB.data_directory\n",
    "analysis_directory = config_file_AEB.analysis_directory  \n",
    "scripts_directory = config_file_AEB.scripts_directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data_set, norm_input, encoding_dim, encoded_activation, input_dropout_pct, dropout_pct, num_epochs, batch_size, n_splits, n_repeats, compute_informative_features, plot_iteration, graph_dir, outFile):\n",
    "    \n",
    "    # format strings for outputting the paramters associated with this run:\n",
    "    summary_string, plotting_string= stats_utils_AEB_110718.format_input_parameters_printing(data_set, norm_input, encoding_dim, encoded_activation,input_dropout_pct,dropout_pct,num_epochs,batch_size,n_splits,n_repeats,compute_informative_features,plot_iteration)\n",
    "\n",
    "    outFile_header='data_set\\tnorm_input\\tencoding_dim\\tencoded_activation\\tinput_dropout_pct\\tdropout_pct\\tnum_epochs\\tbatch_size\\tn_splits\\tn_repeats\\t'\n",
    "\n",
    "    #################\n",
    "    # Load the data # \n",
    "    #################\n",
    "    print('Loading data...')\n",
    "\n",
    "    data_normalized, labels, rskf = load_spAB.load_single_disease(data_set, n_splits, n_repeats, precomputed_kfolds=False)\n",
    "\n",
    "    # rskf = repeated stratified k fold. This contains all the kfold-by-iteration combos. \n",
    "\n",
    "\n",
    "    ###################################################\n",
    "    # iterate through the data kfolds and iterations #\n",
    "    ###################################################\n",
    "\n",
    "    # Create a dictionary to store the metrics of each fold \n",
    "    aggregated_statistics={} # key=n_repeat, values= dictionary with stats\n",
    "    \n",
    "    #needed to change datatype to list bc I was getting - TypeError: 'generator' object is not subscriptable\n",
    "    rskf = list(rskf)\n",
    "\n",
    "    for n_repeat in range(0,len(rskf[0])):\n",
    "        \n",
    "        print('Iteration %s...' %n_repeat)\n",
    "        \n",
    "        aggregated_statistics[n_repeat] = {}\n",
    "        \n",
    "        train_idx = rskf[0][n_repeat]\n",
    "        test_idx = rskf[1][n_repeat]\n",
    "        x_train, y_train = data_normalized[train_idx], labels[train_idx]\n",
    "        x_test, y_test = data_normalized[test_idx], labels[test_idx]\n",
    "    \n",
    "        #standardize the data, mean=0, std=1\n",
    "        if norm_input:\n",
    "            x_train, x_test= stats_utils_AEB_110718.standardize_data(x_train, x_test)\n",
    "    \n",
    "        ###########################################\n",
    "        # set up a model (supervised learning)    #\n",
    "        ###########################################\n",
    "        # note that the model has to be instantiated each time a new fold is started otherwise the weights will not start from scratch. \n",
    "    \n",
    "        input_dim=len(data_normalized[0]) # this is the number of input kmers\n",
    "\n",
    "        model=deep_learning_models.create_supervised_model(input_dim, encoding_dim, encoded_activation,input_dropout_pct, dropout_pct)\n",
    "    \n",
    "        #weightFile = os.environ['HOME'] + '/deep_learning_microbiome/data/weights.txt'\n",
    "       \n",
    "        ##################################################\n",
    "        # Fit the model with the train data of this fold #\n",
    "        ##################################################\n",
    "        history = History()\n",
    "        # history is a dictionary. To get the keys, type print(history.history.keys())\n",
    "        \n",
    "        model.fit(x_train, y_train, \n",
    "                  epochs=num_epochs, \n",
    "                  batch_size=len(x_train), \n",
    "                  shuffle=True,\n",
    "                  validation_data=(x_test, y_test),\n",
    "                  verbose=0,\n",
    "                  callbacks=[history])\n",
    "    \n",
    "        # predict using the held out data\n",
    "        y_pred=model.predict(x_test)\n",
    "        \n",
    "        # save the weights of this model. TODO \n",
    "    \n",
    "        ################################################################\n",
    "        # Compute summary statistics                                   #\n",
    "        ################################################################\n",
    "        # Store the results of this fold in aggregated_statistics\n",
    "        aggregated_statistics = stats_utils_AEB_110718.compute_summary_statistics(y_test, y_pred, history, aggregated_statistics, n_repeat)\n",
    "\n",
    "        # could  plot everything (roc, accuracy vs epoch, loss vs epoch, confusion matrix, precision recall) for each fold, but this will produce a lot of graphs. \n",
    "        if compute_informative_features:\n",
    "            shap_values, shap_values_summed = stats_utils_AEB_110718.compute_shap_values_deeplearning(input_dim, model, x_test)\n",
    "            aggregated_statistics[n_repeat]['shap_values_summed']=shap_values_summed\n",
    "            aggregated_statistics[n_repeat]['shap_values']=shap_values\n",
    "\n",
    "        # also plot:\n",
    "        #shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
    "        #shap.summary_plot(shap_values, X)\n",
    "\n",
    "    ##############################################\n",
    "    # aggregate the results from all the k-folds #\n",
    "    # Print and Plot                             #\n",
    "    ##############################################\n",
    "    print('Aggregating statistics across iterations and printing/plotting...')\n",
    "\n",
    "    stats_utils_AEB_110718.aggregate_statistics_across_folds(aggregated_statistics, rskf, n_splits, outFile, summary_string, plotting_string, outFile_header)\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # Aggregate shap: #\n",
    "    ###################\n",
    "\n",
    "    if compute_informative_features: \n",
    "        print('Computing informative features with Shap...')\n",
    "        stats_utils_AEB_110718.aggregate_shap(aggregated_statistics, rskf)\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    # TSNE visualization                #\n",
    "    # Annamarie                         #\n",
    "    # find the weights of the best fold #\n",
    "    #####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "# parser for the config dict #\n",
    "##############################\n",
    "def parse_config_and_run(config_dict, outFile):\n",
    "    data_sets=config_dict['data_set']\n",
    "    #kmer_sizes=config_dict['kmer_size']\n",
    "    norm_inputs=config_dict['norm_input']\n",
    "    encoding_dims=config_dict['encoding_dim']\n",
    "    encoded_activations=config_dict['encoded_activation']\n",
    "    input_dropout_pcts=config_dict['input_dropout_pct']\n",
    "    dropout_pcts=config_dict['dropout_pct'] \n",
    "    num_epochss=config_dict['num_epochs']\n",
    "    batch_sizes=config_dict['batch_size']\n",
    "    n_splitss=config_dict['n_splits']\n",
    "    n_repeatss=config_dict['n_repeats']\n",
    "    compute_informative_featuress=config_dict['compute_informative_features']\n",
    "    plot_iterations=config_dict['plot_iteration'] \n",
    "    graph_dirs=config_dict['graph_dir'] \n",
    "\n",
    "    for data_set in data_sets:\n",
    "        #for kmer_size in kmer_sizes:\n",
    "        for norm_input in norm_inputs:\n",
    "            for encoding_dim in encoding_dims:\n",
    "                for encoded_activation in encoded_activations:\n",
    "                    for input_dropout_pct in input_dropout_pcts:\n",
    "                        for dropout_pct in dropout_pcts:\n",
    "                            for num_epochs in num_epochss:\n",
    "                                for batch_size in batch_sizes:\n",
    "                                    for n_splits in n_splitss:\n",
    "                                        for n_repeats in n_repeatss:\n",
    "                                            for compute_informative_features in compute_informative_featuress:\n",
    "                                                for plot_iteration in plot_iterations:\n",
    "                                                    for graph_dir in graph_dirs:\n",
    "                                                        \n",
    "                                                        run_model(data_set, \n",
    "                                                                      #kmer_size,\n",
    "                                                                      norm_input,\n",
    "                                                                      encoding_dim,\n",
    "                                                                      encoded_activation,\n",
    "                                                                      input_dropout_pct,\n",
    "                                                                      dropout_pct,\n",
    "                                                                      num_epochs,\n",
    "                                                                      batch_size,\n",
    "                                                                      n_splits,\n",
    "                                                                      n_repeats,\n",
    "                                                                      compute_informative_features,\n",
    "                                                                      plot_iteration,\n",
    "                                                                      graph_dir, \n",
    "                                                                      outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaHIT\n",
      "LOADED DATASET MetaHIT: 110 SAMPLES\n",
      "Qin_et_al\n",
      "LOADED DATASET Qin_et_al: 271 SAMPLES\n",
      "Zeller_2014\n",
      "LOADED DATASET Zeller_2014: 121 SAMPLES\n",
      "LiverCirrhosis\n",
      "LOADED DATASET LiverCirrhosis: 232 SAMPLES\n"
     ]
    }
   ],
   "source": [
    "# Just from when I was making sure species loader still worked\n",
    "data_sets_to_use = [\n",
    "    [['MetaHIT'], ['MetaHIT']],\n",
    "    [['Qin_et_al'], ['Qin_et_al']],\n",
    "    [['Zeller_2014'], ['Zeller_2014']],\n",
    "    [['LiverCirrhosis'], ['LiverCirrhosis']]\n",
    "   ]\n",
    "\n",
    "for data_set in data_sets_to_use:\n",
    "    data_set = data_set[0]\n",
    "    species_cnts, labelz, feats = load_spAB.load_species(data_set)\n",
    "    print(\"LOADED DATASET \" + str(data_set[0]) + \": \" + str(len(species_cnts)) + \" SAMPLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters being tested:\n",
      "MetaHIT\n",
      "Normalize input? True\n",
      "Encoding dim: 8\n",
      "Encoded activation: sigmoid\n",
      "Input dropout percent: 0\n",
      "Dropout percent: 0\n",
      "Num epochs: 400\n",
      "Batch size: 16\n",
      "n_splits (k-folds): 10\n",
      "n_repeats (iterations): 5\n",
      "Compute infromative features with Shap? False\n",
      "Plots for each iteration? False\n",
      "\n",
      "Loading data...\n",
      "MetaHIT\n",
      "Iteration 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pollard/home/abustion/deep_learning_microbiome/scripts/stats_utils_AEB_110718.py:27: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x_train = (x_train - sample_mean) / sample_std\n",
      "/pollard/home/abustion/deep_learning_microbiome/scripts/stats_utils_AEB_110718.py:27: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x_train = (x_train - sample_mean) / sample_std\n",
      "/pollard/home/abustion/deep_learning_microbiome/scripts/stats_utils_AEB_110718.py:29: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x_test = (x_test - sample_mean) / sample_std\n",
      "/pollard/home/abustion/deep_learning_microbiome/scripts/stats_utils_AEB_110718.py:29: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x_test = (x_test - sample_mean) / sample_std\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a5528c4017d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_file_AEB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#for data_set in data_sets_to_use:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparse_config_and_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutFile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"summary_statistics_11718.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c173752fc5c0>\u001b[0m in \u001b[0;36mparse_config_and_run\u001b[0;34m(config_dict, outFile)\u001b[0m\n\u001b[1;32m    161\u001b[0m                                                                       \u001b[0mplot_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                                                                       \u001b[0mgraph_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                                                                       outFile)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c173752fc5c0>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(data_set, norm_input, encoding_dim, encoded_activation, input_dropout_pct, dropout_pct, num_epochs, batch_size, n_splits, n_repeats, compute_informative_features, plot_iteration, graph_dir, outFile)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Store the results of this fold in aggregated_statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0maggregated_statistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats_utils_AEB_110718\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_summary_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregated_statistics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# could  plot everything (roc, accuracy vs epoch, loss vs epoch, confusion matrix, precision recall) for each fold, but this will produce a lot of graphs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep_learning_microbiome/scripts/stats_utils_AEB_110718.py\u001b[0m in \u001b[0;36mcompute_summary_statistics\u001b[0;34m(y_test, y_pred, history, aggregated_statistics, n_repeat)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_summary_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregated_statistics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0my_pred_rounded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mconf_mat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_rounded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \"\"\"\n\u001b[1;32m    612\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 613\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36massert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mallow_nan\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DL_1118/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "config_dict=config_file_AEB.config\n",
    "#for data_set in data_sets_to_use:\n",
    "parse_config_and_run(config_dict, outFile=\"summary_statistics_11718.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
