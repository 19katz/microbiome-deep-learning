{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg') # this suppresses the console for plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import numpy as np\n",
    "from numpy import random, array\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path\n",
    "import pylab\n",
    "import importlib\n",
    "import imp\n",
    "from importlib import reload\n",
    "import gzip\n",
    "import ntpath\n",
    "from Bio import SeqIO\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "from scipy import interp\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize, label_binarize, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import History, TensorBoard\n",
    "from keras import backend as K\n",
    "backend = K.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our private scripts\n",
    "import load_spAB\n",
    "import deep_learning_models\n",
    "import plotting_utils_AEB\n",
    "import stats_utils_AEB_110718\n",
    "import config_file_AEB\n",
    "\n",
    "species_directory = config_file_AEB.species_directory\n",
    "data_directory = config_file_AEB.data_directory\n",
    "analysis_directory = config_file_AEB.analysis_directory  \n",
    "scripts_directory = config_file_AEB.scripts_directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data_set, norm_input, encoding_dim, encoded_activation, input_dropout_pct, dropout_pct, num_epochs, batch_size, n_splits, n_repeats, compute_informative_features, plot_iteration, graph_dir, outFile):\n",
    "    \n",
    "    # format strings for outputting the paramters associated with this run:\n",
    "    summary_string, plotting_string= stats_utils_AEB_110718.format_input_parameters_printing(data_set, norm_input, encoding_dim, encoded_activation,input_dropout_pct,dropout_pct,num_epochs,batch_size,n_splits,n_repeats,compute_informative_features,plot_iteration)\n",
    "\n",
    "    outFile_header='data_set\\tnorm_input\\tencoding_dim\\tencoded_activation\\tinput_dropout_pct\\tdropout_pct\\tnum_epochs\\tbatch_size\\tn_splits\\tn_repeats\\t'\n",
    "\n",
    "    #################\n",
    "    # Load the data # \n",
    "    #################\n",
    "    print('Loading data...')\n",
    "\n",
    "    data_normalized, labels, rskf = load_spAB.load_single_disease(data_set, n_splits, n_repeats, precomputed_kfolds=False)\n",
    "    \n",
    "    print(\"Dimensions of normalized species cnts: \" + str(data_normalized.shape))\n",
    "    print(\"Dimensions of labels: \" + str(labels.shape))\n",
    "    # rskf = repeated stratified k fold. This contains all the kfold-by-iteration combos. \n",
    "\n",
    "\n",
    "    ###################################################\n",
    "    # iterate through the data kfolds and iterations #\n",
    "    ###################################################\n",
    "\n",
    "    # Create a dictionary to store the metrics of each fold \n",
    "    aggregated_statistics={} # key=n_repeat, values= dictionary with stats\n",
    "    \n",
    "    #needed to change datatype to list bc I was getting - TypeError: 'generator' object is not subscriptable\n",
    "    rskf = list(rskf)\n",
    "\n",
    "    for n_repeat in range(0,len(rskf[0])):\n",
    "        \n",
    "        print('Iteration %s...' %n_repeat)\n",
    "        \n",
    "        aggregated_statistics[n_repeat] = {}\n",
    "        \n",
    "        train_idx = rskf[0][n_repeat]\n",
    "        test_idx = rskf[1][n_repeat]\n",
    "        x_train, y_train = data_normalized[train_idx], labels[train_idx]\n",
    "        x_test, y_test = data_normalized[test_idx], labels[test_idx]\n",
    "    \n",
    "        #standardize the data, mean=0, std=1\n",
    "        if norm_input:\n",
    "            x_train, x_test= stats_utils_AEB_110718.standardize_data(x_train, x_test)\n",
    "    \n",
    "        ###########################################\n",
    "        # set up a model (supervised learning)    #\n",
    "        ###########################################\n",
    "        # note that the model has to be instantiated each time a new fold is started otherwise the weights will not start from scratch. \n",
    "    \n",
    "        input_dim=len(data_normalized[0]) # this is the number of input kmers\n",
    "\n",
    "        model=deep_learning_models.create_supervised_model(input_dim, encoding_dim, encoded_activation,input_dropout_pct, dropout_pct)\n",
    "    \n",
    "        #weightFile = os.environ['HOME'] + '/deep_learning_microbiome/data/weights.txt'\n",
    "       \n",
    "        ##################################################\n",
    "        # Fit the model with the train data of this fold #\n",
    "        ##################################################\n",
    "        history = History()\n",
    "        # history is a dictionary. To get the keys, type print(history.history.keys())\n",
    "        \n",
    "        model.fit(x_train, y_train, \n",
    "                  epochs=num_epochs, \n",
    "                  batch_size=len(x_train), \n",
    "                  shuffle=True,\n",
    "                  validation_data=(x_test, y_test),\n",
    "                  verbose=0,\n",
    "                  callbacks=[history])\n",
    "    \n",
    "        # predict using the held out data\n",
    "        y_pred=model.predict(x_test)\n",
    "        \n",
    "        # save the weights of this model. TODO \n",
    "    \n",
    "        ################################################################\n",
    "        # Compute summary statistics                                   #\n",
    "        ################################################################\n",
    "        # Store the results of this fold in aggregated_statistics\n",
    "        aggregated_statistics = stats_utils_AEB_110718.compute_summary_statistics(y_test, y_pred, history, aggregated_statistics, n_repeat)\n",
    "\n",
    "        # could  plot everything (roc, accuracy vs epoch, loss vs epoch, confusion matrix, precision recall) for each fold, but this will produce a lot of graphs. \n",
    "        if compute_informative_features:\n",
    "            shap_values, shap_values_summed = stats_utils_AEB_110718.compute_shap_values_deeplearning(input_dim, model, x_test)\n",
    "            aggregated_statistics[n_repeat]['shap_values_summed']=shap_values_summed\n",
    "            aggregated_statistics[n_repeat]['shap_values']=shap_values\n",
    "\n",
    "        # also plot:\n",
    "        #shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
    "        #shap.summary_plot(shap_values, X)\n",
    "\n",
    "    ##############################################\n",
    "    # aggregate the results from all the k-folds #\n",
    "    # Print and Plot                             #\n",
    "    ##############################################\n",
    "    print('Aggregating statistics across iterations and printing/plotting...')\n",
    "\n",
    "    stats_utils_AEB_110718.aggregate_statistics_across_folds(aggregated_statistics, rskf, n_splits, outFile, summary_string, plotting_string, outFile_header)\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # Aggregate shap: #\n",
    "    ###################\n",
    "\n",
    "    if compute_informative_features: \n",
    "        print('Computing informative features with Shap...')\n",
    "        stats_utils_AEB_110718.aggregate_shap(aggregated_statistics, rskf)\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    # TSNE visualization                #\n",
    "    # Annamarie                         #\n",
    "    # find the weights of the best fold #\n",
    "    #####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "# parser for the config dict #\n",
    "##############################\n",
    "def parse_config_and_run(config_dict, outFile):\n",
    "    data_sets=config_dict['data_set']\n",
    "    #kmer_sizes=config_dict['kmer_size']\n",
    "    norm_inputs=config_dict['norm_input']\n",
    "    encoding_dims=config_dict['encoding_dim']\n",
    "    encoded_activations=config_dict['encoded_activation']\n",
    "    input_dropout_pcts=config_dict['input_dropout_pct']\n",
    "    dropout_pcts=config_dict['dropout_pct'] \n",
    "    num_epochss=config_dict['num_epochs']\n",
    "    batch_sizes=config_dict['batch_size']\n",
    "    n_splitss=config_dict['n_splits']\n",
    "    n_repeatss=config_dict['n_repeats']\n",
    "    compute_informative_featuress=config_dict['compute_informative_features']\n",
    "    plot_iterations=config_dict['plot_iteration'] \n",
    "    graph_dirs=config_dict['graph_dir'] \n",
    "\n",
    "    for data_set in data_sets:\n",
    "        #for kmer_size in kmer_sizes:\n",
    "        for norm_input in norm_inputs:\n",
    "            for encoding_dim in encoding_dims:\n",
    "                for encoded_activation in encoded_activations:\n",
    "                    for input_dropout_pct in input_dropout_pcts:\n",
    "                        for dropout_pct in dropout_pcts:\n",
    "                            for num_epochs in num_epochss:\n",
    "                                for batch_size in batch_sizes:\n",
    "                                    for n_splits in n_splitss:\n",
    "                                        for n_repeats in n_repeatss:\n",
    "                                            for compute_informative_features in compute_informative_featuress:\n",
    "                                                for plot_iteration in plot_iterations:\n",
    "                                                    for graph_dir in graph_dirs:\n",
    "                                                        \n",
    "                                                        run_model(data_set, \n",
    "                                                                      #kmer_size,\n",
    "                                                                      norm_input,\n",
    "                                                                      encoding_dim,\n",
    "                                                                      encoded_activation,\n",
    "                                                                      input_dropout_pct,\n",
    "                                                                      dropout_pct,\n",
    "                                                                      num_epochs,\n",
    "                                                                      batch_size,\n",
    "                                                                      n_splits,\n",
    "                                                                      n_repeats,\n",
    "                                                                      compute_informative_features,\n",
    "                                                                      plot_iteration,\n",
    "                                                                      graph_dir, \n",
    "                                                                      outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure loader is working\n",
    "\n",
    "data_set = ['MetaHIT']\n",
    "species_cnts, labelz, feats = load_spAB.load_species(data_set)\n",
    "print(\"LOADED DATASET \" + str(data_set) + \": \" + str(len(species_cnts)) + \" SAMPLES\")\n",
    "\n",
    "# Checking the data\n",
    "np.count_nonzero(species_cnts==0, axis = 1) # a lot of nonzeros\n",
    "np.any(np.isnan(species_cnts)) # returns False\n",
    "np.all(np.isfinite(species_cnts)) # returns True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict=config_file_AEB.config\n",
    "#for data_set in data_sets_to_use:\n",
    "parse_config_and_run(config_dict, outFile=\"summary_statistics_11718.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set='MetaHIT'\n",
    "print('Loading data...')\n",
    "\n",
    "data_normalized, labels, rskf = load_spAB.load_single_disease(data_set, n_splits=10, n_repeats=5, precomputed_kfolds=False)\n",
    "#data_normalized = data_normalized[:, ~np.all(data_normalized == 0, axis=0)]    \n",
    "print(\"Dimensions of normalized species cnts: \" + str(data_normalized.shape))\n",
    "print(\"Dimensions of labels: \" + str(labels.shape))\n",
    "\n",
    "rskf = list(rskf)\n",
    "\n",
    "for n_repeat in range(0,len(rskf[0])):\n",
    "    train_idx = rskf[0][n_repeat]\n",
    "    test_idx = rskf[1][n_repeat]\n",
    "    x_train, y_train = data_normalized[train_idx], labels[train_idx]\n",
    "    x_test, y_test = data_normalized[test_idx], labels[test_idx]\n",
    "    \n",
    "    #standardize the data, mean=0, std=1\n",
    "    #x_train, x_test= stats_utils_AEB_110718.standardize_data(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set=['MetaHIT']\n",
    "allowed_labels=['0','1']\n",
    "species_cnts, labels, features = load_spAB.load_species(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of features that are zero for all of the samples\n",
    "species_cnts = species_cnts[:, ~np.all(species_cnts == 0, axis=0)]\n",
    "\n",
    "\n",
    "labels=np.asarray(labels)\n",
    "labels=labels.astype(np.int)\n",
    "\n",
    "data_normalized = species_cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf = rskf.split(data_normalized, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(rskf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf = list(rskf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(0,len(rskf[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_repeat in range(0,len(rskf[0])):\n",
    "    train_idx = rskf[0][n_repeat]\n",
    "    test_idx = rskf[1][n_repeat]\n",
    "    x_train, y_train = data_normalized[train_idx], labels[train_idx]\n",
    "    x_test, y_test = data_normalized[test_idx], labels[test_idx]\n",
    "    \n",
    "    sample_mean = x_train.mean(axis=0)\n",
    "    sample_std = x_train.std(axis=0)\n",
    "    \n",
    "    #x_train = (x_train - sample_mean) / sample_std\n",
    "    \n",
    "    #x_test = (x_test - sample_mean) / sample_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test first repeat\n",
    "len(rskf[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(x_train==0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(sample_mean==0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(sample_std==0, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set=['MetaHIT']\n",
    "allowed_labels=['0','1']\n",
    "species_cnts, labels, features = load_spAB.load_species(data_set)\n",
    "    \n",
    "# get rid of features that are zero for a fraction of the samples\n",
    "species_cnts = species_cnts[:, ~np.all(species_cnts == 0, axis=0)]\n",
    "\n",
    "labels=np.asarray(labels)\n",
    "labels=labels.astype(np.int)\n",
    "\n",
    "data_normalized = species_cnts\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=5)\n",
    "rskf = rskf.split(data_normalized, labels)\n",
    "rskf = list(rskf)\n",
    "\n",
    "for n_repeat in range(0,len(rskf[0])):\n",
    "    train_idx = rskf[0][n_repeat]\n",
    "    test_idx = rskf[1][n_repeat]\n",
    "    x_train, y_train = data_normalized[train_idx], labels[train_idx]\n",
    "    x_test, y_test = data_normalized[test_idx], labels[test_idx]\n",
    "    \n",
    "    #x_train, x_test = stats_utils_AEB_110718.standardize_data(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = list(np.count_nonzero(data_normalized==0, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevs = np.count_nonzero(data_normalized==0, axis=0)\n",
    "\n",
    "prevs2 = [i for i in zeros if i >= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting hist without kde\n",
    "ax = sns.distplot(prevs, kde=False)\n",
    "# Labels\n",
    "plt.title(\"MetaHIT species data\")\n",
    "plt.xlabel(\"Number of zero entries\\n(note: there are 110 samples)\")\n",
    "plt.ylabel(\"Taxa (i.e. column) count\")\n",
    "\n",
    "# plot the zoomed portion\n",
    "sub_axes1 = plt.axes([.25, .5, .38, .3]) \n",
    "sns.distplot(prevs2,kde=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.any(np.isnan(x_train))) # returns False\n",
    "print(np.all(np.isfinite(x_train))) # returns True\n",
    "\n",
    "print(np.any(np.isnan(x_test))) # returns False\n",
    "print(np.all(np.isfinite(x_test))) # returns True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = stats_utils_AEB_110718.standardize_data(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs = data_normalized.std(axis=0) / data_normalized.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(covs, bins=20, kde=False)\n",
    "plt.title(\"Coefficient of Variation\\n(MetaHIT dataset)\")\n",
    "plt.xlabel(\"COV score\")\n",
    "plt.ylabel(\"No. features (taxa)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
