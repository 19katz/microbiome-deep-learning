#!/bin/bash                         #-- what is the language of this shell
#                                  #-- Any line that starts with #$ is an instruction to SGE
#$ -S /bin/bash                     #-- the shell for the job
#$ -o output_error_directory       #-- output directory (fill in)
#$ -e output_error_directory       #-- error directory (fill in)
#$ -cwd                            #-- tell the job that it should start in your working directory
#$ -r y                            #-- tell the system that if a job crashes, it should be restarted
#$ -j y                            #-- tell the system that the STDERR and STDOUT should be joined
#$ -l mem_free=1G                  #-- submits on nodes with enough free memory (required)
#$ -l arch=linux-x64               #-- SGE resources (CPU type)
#$ -l netapp=2G,scratch=2G         #-- SGE resources (home and scratch disks)
#$ -l h_rt=336:00:00               #-- runtime limit (see above; this requests 24 hours)
#$ -t 1-292                        #-- remove first '#' to specify the number of
#$ -tc 50                                   #-- tasks if desired (see Tips section)

readarray files < ~/shattuck/metagenomic_fastq_files/IGC/accessions_single.txt
files=(null ${files[@]}) # this pads the file with an extra line in the beginning. 
file=${files[$SGE_TASK_ID]}
echo $file

dir=/pollard/shattuck0/snayfach/metagenomes/IGC/fastq

echo $file
dataset=IGC

for kmer_size in 5 6 7 8 10; do
    mkdir deep_learning_microbiome/data/${kmer_size}mers_jf/${dataset}

    jellyfish count <(zcat ${dir}/${file}.fastq.gz) -m ${kmer_size} -s 100M -t 2 -C -F 2 -o ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${dataset}/${file}_${kmer_size}mer.jf 

    jellyfish dump ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${dataset}/${file}_${kmer_size}mer.jf | grep '>' | gzip > ~/deep_learning_microbiome/data/${kmer_size}mers_jf/${dataset}/${file}_${kmer_size}mer.gz

done

